
## Инференс LLM: две стадии

1) **Prefill** — прогон всего входного префикса, вычисление и сохранение KV‑Cache. Требует большого количества вычислений. Compute-bound.

2) **Decode** — генерация токенов по одному с переиспользованием KV‑Cache. Вычислений меньше, критична задержка между токенами. Memory-bound.

---

## Метрики скорости

* **Prefill:** TTFT (time to first token) — для коротких промптов <1с, для длинных — до десятков секунд
* **Decode:** ITL (inter token latency) — десятки миллисекунд на токен (чем меньше, тем лучше)

---

## Пример

Запрос в чат → Prefill (системный промт + запрос) → KV-Cache сохраняется → Decode генерирует токены по одному и отправляет пользователю.

---

## Проблема

Один сервер обрабатывает много запросов (prefill + decode). Один длинный prefill запрос на 10 секунд утилизирует GPU, блокируя все decode запросы. ITL проседает для всех пользователей.

---

## Решение: Disaggregated Prefill

Разделение Prefill и Decode на разные кластеры:
- **Prefill-кластер** — прогон префиксов, выдача KV‑Cache
- **Decode-кластер** — стриминг токенов с низкой задержкой

Стабилизирует ITL и изолирует длинные prefill запросы от decode.

---

## Передача KV‑Cache

Через быстрое хранилище/транспорт: 3FS, RDMA, NVMe/SSD. Минимум копирований и задержки.

---

## Итог

- Две стадии: **Prefill** (compute-bound) и **Decode** (memory-bound)
- **Disaggregated Prefill** — разделение на отдельные кластеры для Prefill и Decode
- Преимущества: стабилизация ITL, изоляция нагрузки, оптимизация ресурсов

---
---


## Квиз

1) Когда нужен Disaggregated Prefill?

* Студенческие проекты
* Исследовательские проекты
* Большие сервисы инференса

**Ответ:** Для **больших сервисов инференса**, где критична стабильность ITL. Для студенческих и исследовательских проектов обычно достаточно единого сервера.

2) Время хранения KV‑Cache

* Секунды
* Минуты
* Часы
* Дни

**Ответ:** От нескольких минут до часов — зависит от сервиса, размера контекста и политики управления памятью.
