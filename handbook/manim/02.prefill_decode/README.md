
## Инференс LLM делится на две разные стадии:

1) Prefill — модель прогоняет ВЕСЬ входной префикс, считает и сохраняет KV‑Cache. Этот этап требует большого количества вычислений. Compute-bound.

2) Decode — модель выдаёт по одному новому токену, переиспользуя KV‑Cache. На этом
этапе количество вычислений сильно меньше по сравнению с Prefill, но важна задержка между сгенерированными токенами. Memory-bound.

---

## Какие есть метрики скорости для Prefill / Decode?

* Prefill: TTFT — time to first token (время до первого токена)
* Decode: ITL — inter token latency (задержка между токенами)

Какие значения считаются хорошими? Для коротких промптов TTFT обычно целится в <1с для длинных может занимать порядка десяти секунд. ITL для хорошего UX — десятки миллисекунд на токен (чем меньше, тем «живее» поток).

---

## Рассмотрим пример:

Написали запрос в чат. Чтобы его обработать, ChatGPT берет системный промт и ваш запрос и запускает для него Prefill. Результат работы - KV-Cache сохраняется в памяти.

Дальше идет более легковесная процедура декодирования, токен по одному генерируется и
отправляется пользователю / по API.

---

## Какие проблемы могут тут возникнуть?

Представим ситуацию: один сервер, который обрабатывает много маленьких запросов на prefill и decode. Пусть одновременно один сервер может обрабатывать 10 таких запросов.
С хорошей задержкой для всех пользователей.

И представим, что в какой-то момент один из пользователей отправляет большой контекст
для обработки. Этот запрос на 10 секунд полностью утилизирует GPU так, что все остальные decode запросы ничего не могут сделать за это время. Из-за этого ITL сильно проседает

---

Как починить? - Disaggregated Prefill.

Разносим Prefill и Decode на разные серверы (кластеры серверов). Prefill-кластер занимается только прогоном префиксов и выдаёт KV‑Cache, Decode-кластер стримит токены
с низкой задержкой. Так мы стабилизируем ITL и не позволяем одному длинному
промпту сломать пользовательский опыт всех остальных.

<!-- Кроме того, можно запускать разные этапы на разных GPU - для Prefill требуется большая пропускная способность. А для decode - задержка. -->

---

Как передаём KV‑Cache с Prefill на Decode?

Через быстрое общее хранилище/транспорт: 3FS, RDMA, NVMe/SSD. Цель — минимум копирований
и задержки при доставке KV‑тензоров.

---

Сколько хранится KV‑Cache?

От нескольких минут до часов — зависит от сервиса для инференса, которым вы пользуетесь, размера контекста и политики управления памятью.

---

Итог:

- При обработке запроса LLM есть две стадии: Prefill и Decode
- Prefill — предварительная обработка всего входного префикса с вычислением и сохранением KV‑Cache (compute-bound)
- Decode — генерация токенов по одному с переиспользованием KV‑Cache (memory-bound)
- Disaggregated Prefill — это архитектурный подход, при котором выделяют отдельные группы серверов для обработки Prefill и Decode. Это помогает:
  - Стабилизировать ITL (inter token latency) для decode запросов
  - Изолировать длинные prefill запросы от decode запросов
  - Оптимизировать использование ресурсов под разные типы нагрузки

---
---


Квиз:

Когда нужен Disaggregated Prefill?

* Студенческие проекты
* Исследовательские проекты
* Большие сервисы инференса

**Ответ:** Disaggregated Prefill нужен в первую очередь для **больших сервисов инференса**, где критична стабильность ITL и изоляция разных типов нагрузки. Для студенческих и исследовательских проектов обычно достаточно единого сервера.
