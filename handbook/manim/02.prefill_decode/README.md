Как измериют скорость работы ChatGPT?

Вообще говоря, есть две метрики

TTFT - time to first token

ITL - inter token latency

Какие значения для этих метрик считаются хорошими?
Какие значения для чатгпт / дипсика?

---

Но почему для этого запроса придумали две отдельные метрики?

Оказывается, что "сгенерировать" первый токен для LLM
сильно сложнее, потому что ей надо посчитать KV-Cache для
всех новых данных. И если мы передали в LLM большой запрос,
это может занять несколько секунд или даже десятков секунд.

Эта стадия называется Prefill. Результатом prefill
является KV-Cache для префикса.

Когда KV-Cache для префикса посчитали, запускается стадия Decode - на этой 

---

Disaggregated prefill

Представим, что у нас есть один сервер, который обрабатывает все запросы.

Decode запросы чувствительны к задержке, но требуют не очень много вычислений.
И если на сервер приходит несколько или даже много параллельных Decode запросов,
он выдерживает нагрузку и все Decode запросы обслуживает с хорошей задержкой.

Но вдруг, неожиданно, приходит один большой prefill запрос на большое количество токенов
Prefill очень требователен к ресурсам и утилизирует все доступные вычислительные ресурсы.

Это приводит к тому, что обрабатываемые decode запросы начинают отвечать медленнее, увеличивается задержка.

---

Как решить эту проблему? И поддерживать стабильно маленькую задержку для Decode запросов?

Нужно разнести нагрузку Prefill и Decode запросов на разные серверы. Тогда


---

Сколько хранится KV-Cache?

---

Как передаются KV-Cache'ы? С prefill на decode кластеры?

3FS: RDMA / SSD

---

Chunked prefill - как 

---
