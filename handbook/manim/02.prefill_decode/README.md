Как измеряют «скорость» LLM вроде ChatGPT?

Есть две ключевые метрики:

TTFT — time to first token (время до первого токена)

ITL — inter token latency (задержка между токенами)

Какие значения считаются хорошими? Для коротких промптов TTFT обычно целится в <1–2 с, для длинных — в считанные секунды. ITL для хорошего UX — десятки миллисекунд на токен (чем меньше, тем «живее» поток). В реальности у ChatGPT/DeepSeek цифры зависят от модели, длины промпта и нагрузки, но ощущение «быстро/медленно» почти всегда определяется именно этими двумя метриками.

---

Почему нужны ДВЕ метрики?

Потому что генерация делится на две разные стадии с разной «физикой»:

1) Prefill — модель прогоняет ВЕСЬ входной префикс, считает и сохраняет KV‑Cache.
Это самая тяжёлая часть: если промпт длинный, TTFT растёт.

2) Decode — модель выдаёт по одному новому токену, переиспользуя KV‑Cache.
Это легче вычислительно, но критично к задержкам (формирует ITL).

---

Интуитивно: первый токен ждём дольше для генерации KV-Cache всего префикса (Prefill).
Дальше наступает этап Decode и текст «льётся» равномерно, генерируется токен за токеном.

---

Сценарий нагрузки: один сервер, много мелких Decode и один огромный Prefill.

Decode-запросы требуют мало FLOPs, но чувствительны к задержке — сервер их тянет.
Приходит один тяжёлый Prefill на большой промпт — он выедает все ресурсы (GPU/внимание),
и мелкие Decode внезапно начинают «тормозить»: ITL ухудшается, UX рушится.

---

Как починить? Disaggregated Prefill.

Разносим Prefill и Decode на разные пулы/кластеры. Prefill-кластер занимается
только прогоном префиксов и выдаёт KV‑Cache, Decode-кластер стримит токены
с низкой задержкой. Так мы стабилизируем ITL и не позволяем одному длинному
промпту сломать пользовательский опыт всех остальных.


---

Как передаём KV‑Cache с Prefill на Decode?

Через быстрое общее хранилище/транспорт: 3FS, RDMA, NVMe/SSD. Цель — минимум копирований
и задержки при доставке KV‑тензоров. На практике выбирают архитектуру под сеть/железо.

---

Сколько хранится KV‑Cache?

От минут до часов — зависит от политики кэша, памяти и бюджета. Дольше — дороже,
но дешевле, чем каждый раз прогонять длинный префикс заново.

---

Итог:

- При обработке запроса LLM есть две стадии: Prefill, Decode
- Prefill
- Disaggregated Prefill - это когда выделяют группы серверов отдельно для обработки Prefill и отдельно для обработки стадии Decode. Это помогает:

Разделяйте Prefill/Decode и ускоряйте путь KV‑Cache.
