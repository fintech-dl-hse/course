## Интро

Инференс LLM делится на две стадии:
* Prefill - считает и сохраняет KV-Cache
* Decode - используя KV-Cache генерирует новые токены.

Этот мем зря обижает Decode стадию - это такая же важная стадия
работы LLM, как и Prefill.

---


Но без Prefill был бы невозможен Decode. Потому что генерация без KV-Cache - это
очень долго. Про это мы говорили в предыдущем видео.
В прошлом видео мы говорили, почему KV-Cache так важен для быстрой генерации, но
опустили этап вычисления KV-Cache

Давайте рассмотрим пример.

## Пример

Запрос в чат
        → Prefill (системный промт + запрос. Compute-bound.)
        → KV-Cache сохраняется
        → Decode генерирует токены по одному и отправляет пользователю (Вычислений меньше, критична задержка между токенами. Memory-bound.)

---

## Метрики скорости

* **Prefill:** TTFT (time to first token) — секунды, максимум десятки секунд
* **Decode:** ITL (inter token latency) — десятки миллисекунд на токен

---

## Проблема

Один сервер обрабатывает много запросов (Prefill + Decode). Один длинный prefill запрос на 10 секунд утилизирует GPU, блокируя все decode запросы. ITL проседает для всех пользователей.

---

## Решение: Disaggregated Prefill

Разделение Prefill и Decode на разные кластеры:
- **Prefill-кластер** — прогон префиксов, выдача KV‑Cache
- **Decode-кластер** — стриминг токенов с низкой задержкой

Стабилизирует ITL и изолирует длинные Prefill запросы от Decode.

---

## Передача KV‑Cache

Через быстрое хранилище/транспорт: 3FS, RDMA, NVMe/SSD. Минимум копирований и задержки.

---

## Итог

- Две стадии: **Prefill** (compute-bound) и **Decode** (memory-bound)
- **Disaggregated Prefill** — разделение на отдельные кластеры для Prefill и Decode
- Преимущества: стабилизация ITL, изоляция нагрузки, оптимизация ресурсов

---
---


## Квиз

1) Когда нужен Disaggregated Prefill?

* Студенческие проекты
* Исследовательские проекты
* Большие сервисы инференса

**Ответ:** Для **больших сервисов инференса**, где критична стабильность ITL. Для студенческих и исследовательских проектов обычно достаточно единого сервера.

2) Время хранения KV‑Cache

* Секунды
* Минуты
* Часы
* Дни

**Ответ:** От нескольких минут до часов — зависит от сервиса, размера контекста и политики управления памятью.

3) Decode - это memory-bound задача. Это означает, что большую часть времени GPU ...

* ... ожидает загрузку/запись данных (веса модели и KV-Cache) из GPU-памяти (HBM)
* ... делает вычисления (умножение матриц, операции внимания, активации)
* ... синхронизируется с другими процессами на сервере
* ... читает данные с диска (SSD/NVMe)

**Ответ:** GPU **ожидает загрузку/запись данных из GPU-памяти (HBM)**. В memory-bound задачах узкое место — пропускная способность памяти, а не вычислительная мощность. Вычисления выполняются быстро, но GPU простаивает, ожидая данные из памяти.

4) Выберите правильное утверждение:

KV-Cache растет во время стадии(ий) ..

* Только стадии Prefill
* Только стадии Decode
* Обе стадии Prefill, Decode увеличивают KV-Cache