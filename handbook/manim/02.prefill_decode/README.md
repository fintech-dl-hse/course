
## Инференс LLM делится на две разные стадии:

1) Prefill — модель прогоняет ВЕСЬ входной префикс, считает и сохраняет KV‑Cache. Этот этап требует большое количество вычислений. Compute-bound.

2) Decode — модель выдаёт по одному новому токену, переиспользуя KV‑Cache. На этом
этапе количество вычислений сильно меньше по сравнению с Prefill, но важна скорость задержка между сгенерированными токенами. Memory-bound.

---

## Какие есть метрики скорости для Prefill / Decode?

* Prefill: TTFT — time to first token (время до первого токена)
* Decode: ITL — inter token latency (задержка между токенами)

Какие значения считаются хорошими? Для коротких промптов TTFT обычно целится в <1с для длинных может занимать порядка десяти секунд. ITL для хорошего UX — десятки миллисекунд на токен (чем меньше, тем «живее» поток).

---

## Рассмотрим пример:

Написали запрос в чат. Чтобы его обработать, ChatGPT берет системный промт и ваш запрос и запускает для него Prefill. Результат работы - KV-Cache сохраняется в памяти.

Дальше идет более легковесная процедура декодирования, токен по одному генерируется и
отправляется пользователю / по API.

---

## Какие проблемы могут тут возникнуть?

Представим ситуацию: один сервер, который обрабатывает много маленьких запросов на prefill и decode. Пусть одновременно один сервер может обрабатывать 10 таких запросов.
С хорошей задержкой для всех пользователей.

И представим, что в какой-то момент один из пользователей отправляет большой контекст
для обработки. Этот запрос на 10 секунд полностью утилизирует GPU так, что все остальные decode pапросы ничего не могут сделать за это время. Из-за этого ITL сильно проседает

---

Как починить? - Disaggregated Prefill.

Разносим Prefill и Decode на разные серверы (кластеры серверов). Prefill-кластер занимается только прогоном префиксов и выдаёт KV‑Cache, Decode-кластер стримит токены
с низкой задержкой. Так мы стабилизируем ITL и не позволяем одному длинному
промпту сломать пользовательский опыт всех остальных.

<!-- Кроме того, можно запускать разные этапы на разных GPU - для Prefill требуется большая пропускная способность. А для decode - задержка. -->

---

Как передаём KV‑Cache с Prefill на Decode?

Через быстрое общее хранилище/транспорт: 3FS, RDMA, NVMe/SSD. Цель — минимум копирований
и задержки при доставке KV‑тензоров.

---

Сколько хранится KV‑Cache?

От нескольких минут до часов — зависит от сервиса для инфереса, которым вы пользуетесь.

---

Итог:

- При обработке запроса LLM есть две стадии: Prefill, Decode
- Prefill
- Disaggregated Prefill - это когда выделяют группы серверов отдельно для обработки Prefill и отдельно для обработки стадии Decode. Это помогает:

Разделяйте Prefill/Decode и ускоряйте путь KV‑Cache.

---
---


Квиз:

Когда нужен Disaggregated Prefill?

* Студенческие проекты
* Исследовательские проекты
* Большие сервисы инференса
