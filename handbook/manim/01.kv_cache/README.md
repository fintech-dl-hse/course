
Давайте же разберемся, что такое KV-Cache?

Но сначала вспомним, как работает генерация в трансформерах.

---

Tрансформер - это авторегрессионная модель.
1) Каждый шаг - генерируем новый токен (TODO highlight)
2) И каждый новый токен зависит от всех предыдущих (TODO highlight)

$$ x_t​∼P_{\theta}​(x_t​ ∣ x_1, x_2, ..., x_{t-1}​) $$

---

Как бы выглядела генерация без KV-Cache?

На каждом шаге перевычисляются эмбэддинги для вообще всех
токенов. Поэтому количество вычислений
растет **квадратично** с длинной последовательности.

<!-- *Решение:*
Стандартная практика сэкономить вычисления - это закэшировать результат. -->

---

**Генерация с KV-Cache**

При генерации с KV-Cache мы переиспользуем эмбэддинги, вычесленные на предыдущем шаге.
Это требует дополнительной памяти, но зато кол-во вычислений растет **линейно**.

---

Итого получаем трейдофф: память / вычисления.
Но тк квадратичная чаложность по вычислениям - это очень плохо,
на практике для генерации всегда используется KV-Cache.

---

Кокретный пример:

8B моделька на для 100k токенов требует 13Gb KV-Cache.

Это много, но посмотрите на скорость генерации. С KV-Cache скорость
генерации выше на 2-3 порядка.

<!-- Для Llama3 на 8B параметров размер KV-Cache для 100k токенов равен 13Gb. -->
<!-- Это довольно много с учетом того, что сама модель на GPU занимает 16GB. -->

<!-- 100k токенов - это 150 страниц текста A4 12pt однострочный интервал.
Это очень большой размер контекста. Но современные модели часто работают на больших длинных контекста:
например, для работы с кодовыми базами или документацией часто нужна поддержка большой длинны контекста или
контекст используется для ризонинга размышляющих моделей. -->



<!-- Это много, но давате посмотрим на скорость генеерациии в табличке.
С KV-Cache скорость генерации падает незначительно с увеличением контекста.
А без KV-Cache скорость генерации падает очень быстро. И для контекста 100k
токенов генерация без KV-Cache в 500 размедленеее, чем генерация с KV-Cache. -->

---

Можно ли использовать KV-Cache для ускорения обучения LLM?
Краткий ответ - нет.
А более подробно, почему это так мы разберем в следующем видео.

---

Квиз:

* Если рассмотреть асиимптотику по вычислениям для генерации с KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(1)
- O(n)
- O(n^2)
- O(n^3)

Правильный ответ - `O(n^2)`, потому что если для каждого токена требуется `O(n)` вычислений, то для генерации `n` токенов, потребуется `O(n^2)`.

* Если рассмотреть асиимптотику по вычислениям для генерации БЕЗ KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(n)
- O(n^2)
- O(n^3)
- O(n^4)

Правильный ответ - `O(n^3)`, потому что если для каждого токена требуется `O(n^2)` вычислений, то для генерации `n` токенов, потребуется `O(n^3)`.

