
Настало время разобраться, что такое KV-Cache?

Но сначала вспомним, как работает генерация в трансформерах.

---

Tрансформер - это авторегрессионная модель.
1) За один шаг - генерируем новый токен
2) Этот новый токен зависит от всех предыдущих

$$ x_t​∼P_{\theta}​(x_t​ \mid x_1, x_2, ..., x_{t-1}​) $$

---

Как бы выглядела генерация без KV-Cache?

На каждом шаге перевычисляются эмбэддинги для вообще всех
токенов. Поэтому количество вычислений
растет **квадратично** с длинной последовательности.

---

При генерации с KV-Cache мы переиспользуем эмбэддинги, вычесленные на предыдущем шаге.
Это требует дополнительной памяти, но зато кол-во вычислений растет **линейно**.

---

Итого получаем трейдофф: память / вычисления.
Квадратичеые вычисления - это боль, поэтому
на практике для генерации всегда используется KV-Cache.

---

Кокретный пример:

8B моделька на для 100k токенов требует 13Gb KV-Cache.

Это много, скорость генерации выше на 2-3 порядка.

Поэтому KV-Cache - бро

---
---


Квиз:

* Если рассмотреть асиимптотику по вычислениям для генерации с KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(1)
- O(n)
- O(n^2)
- O(n^3)

Правильный ответ - `O(n^2)`, потому что если для каждого токена требуется `O(n)` вычислений, то для генерации `n` токенов, потребуется `O(n^2)`.

* Если рассмотреть асиимптотику по вычислениям для генерации БЕЗ KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(n)
- O(n^2)
- O(n^3)
- O(n^4)

Правильный ответ - `O(n^3)`, потому что если для каждого токена требуется `O(n^2)` вычислений, то для генерации `n` токенов, потребуется `O(n^3)`.

