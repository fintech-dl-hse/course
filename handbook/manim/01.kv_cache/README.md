
В этом видео мы разберем, что такое KV-Cache, зачем он нужен и как работает.

---

**Recap:**
Tрансформер - это модель авторегрессивной генерации.
Это значит, что:
  - За один шаг мы генерируем обычно один токен
  - Каждый новый токен зависит от последовательности предыдущих

$$ x_t​∼P_{\theta}​(x_t​ ∣ x_1, x_2, ..., x_{t-1}​) $$

---

**Генерация без KV-Cache**

На каждом шаге перевычисляются эмбэддинги для вообще всех
токенов, не только для последнего. Поэтому количество вычислений
растет **квадратично** с длинной последовательности.

*Решение:*
Стандартная практика сэкономить вычисления - это закэшировать результат.

---

**Генерация с KV-Cache**

Количество вычислений растет **линейно**. Но требуется дополнительная память для кэширования эмбэддингов, вычисленных на предыдущих шагах.

---

Итого:

|             | Per Layer Memory     | Per token Computation |
| --- | --- | --- |
| No KV-Cache  |  O(1)          | O(n^2)       |
| KV-Cache     |  O(n)          | O(n)         |


---

Давайте рассмотрим это на конкретном примере

Для Llama3 на 8B параметров размер KV-Cache для 100k токенов равен 13Gb.
Это довольно много с учетом того, что сама модель на GPU занимает 16GB.

100k токенов - это 150 страниц текста A4 12pt однострочный интервал.
Это очень большой размер контекста. Но современные модели часто работают на больших длинных контекста:
например, для работы с кодовыми базами или документацией часто нужна поддержка большой длинны контекста или
контекст используется для ризонинга размышляющих моделей.

Давайте теперь посмотрим на скорость работы LLM на разных длиннах контекста.
Можно заметить, что скорость генерации с KV-Cache падает незначительно даже при
больших длиннах контекста. В то время как без KV-Cache скорость генерации заметно падает
с увеличением длинны контекста.

И для контекста 100k токенов генерация без KV-Cache в 500 размедленеее, чем генерация с KV-Cache.

---

Многие могут подумать, если KV-Cache так сильно ускоряет генерацию, можно ли использовать его
для ускорения обучения LLM? Краткий ответ - нет.
А более подробно, почему это так мы разберем в следующем видео.

---

Квиз:

* Если рассмотреть асиимптотику по вычислениям для генерации с KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(1)
- O(n)
- O(n^2)
- O(n^3)

Правильный ответ - `O(n^2)`, потому что если для каждого токена требуется `O(n)` вычислений, то для генерации `n` токенов, потребуется `O(n^2)`.

* Если рассмотреть асиимптотику по вычислениям для генерации БЕЗ KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(n)
- O(n^2)
- O(n^3)
- O(n^4)

Правильный ответ - `O(n^3)`, потому что если для каждого токена требуется `O(n^2)` вычислений, то для генерации `n` токенов, потребуется `O(n^3)`.


* 100k токенов - сколько это в листах A4 (12pt, одинарный интервал)?

- 15 страниц
- 150 страниц
- 1500 страниц
- 15000 страниц

100k токенов ~= 75k слов, на одной странице 500 слов