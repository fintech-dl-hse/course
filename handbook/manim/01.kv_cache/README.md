
**Recap:**
Tрансформер - это модель авторегрессивной генерации.
Это значит, что:
  - За один шаг мы генерируем обычно один токен
  - Каждый новый токен зависит от последовательности предыдущих

$$ x_t​∼P_{\theta}​(x_t​ ∣ x_1, x_2, ..., x_{t-1}​) $$

---

**Генерация без KV-Cache**

На каждом шаге перевычисляются эмбэддинги для вообще всех
токенов, не только для последнего. Поэтому количество вычислений
растет **квадратично** с длинной последовательности.

*Решение:*
Стандартная практика сэкономить вычисления - это закэшировать результат.

---

**Генерация с KV-Cache**

Количество вычислений растет **линейно**. Но требуется дополнительная память для кэширования эмбэддингов, вычисленных на предыдущих шагах.

---

Итого:

|             | Per Layer Memory     | Per token Computation |
| --- | --- | --- |
| No KV-Cache  |  O(1)          | O(n^2)       |
| KV-Cache     |  O(n)          | O(n)         |


**TODO** пример для реальной модельки, на сколько KV-Cache ускоряет генерацию и сколько использует памяти.

---

Блиц:
* Можно ли использовать KV-Cache во время обучения?
    KV-Cache используется только во время инференса. Во время обучения трансформер не генерирует новые токены. А эмбэддинги для входной последовательности вычисляются параллельно в `prefill` режиме. Об этом мы расскажем в следующем видео.

---

Квиз:

* Если рассмотреть асиимптотику по вычислениям для генерации с KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(1)
- O(n)
- O(n^2)
- O(n^3)

Правильный ответ - `O(n^2)`, потому что если для каждого токена требуется `O(n)` вычислений, то для генерации `n` токенов, потребуется `O(n^2)`.

* Если рассмотреть асиимптотику по вычислениям для генерации БЕЗ KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(n)
- O(n^2)
- O(n^3)
- O(n^4)

Правильный ответ - `O(n^3)`, потому что если для каждого токена требуется `O(n^2)` вычислений, то для генерации `n` токенов, потребуется `O(n^3)`.


* 100k токенов - сколько это в листах A4 (12pt, одинарный интервал)?

- 15 страниц
- 150 страниц
- 1500 страниц
- 15000 страниц

100k токенов ~= 75k слов, на одной странице 500 слов