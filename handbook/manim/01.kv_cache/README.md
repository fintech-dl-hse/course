
Давайте же разберемся, что такое KV-Cache?

Но сначала вспомним, как работает генерация в трансформерах.

---

Tрансформер - это авторегрессионная модель.
1) Каждый шаг - генерируем новый токен (TODO highlight)
2) И каждый новый токен зависит от всех предыдущих (TODO highlight)

$$ x_t​∼P_{\theta}​(x_t​ ∣ x_1, x_2, ..., x_{t-1}​) $$

---

Как бы выглядела генерация без KV-Cache?

На каждом шаге перевычисляются эмбэддинги для вообще всех
токенов. Поэтому количество вычислений
растет **квадратично** с длинной последовательности.

---

При генерации с KV-Cache мы переиспользуем эмбэддинги, вычесленные на предыдущем шаге.
Это требует дополнительной памяти, но зато кол-во вычислений растет **линейно**.

---

Итого получаем трейдофф: память / вычисления.
Квадратичная саложность по вычислениям - это очень плохо,
на практике для генерации всегда используется KV-Cache.

---

Кокретный пример:

8B моделька на для 100k токенов требует 13Gb KV-Cache.

Это много, но посмотрите на скорость генерации. С KV-Cache скорость
генерации выше на 2-3 порядка.

Именно поэтому KV-Cache так необходимы.

<!-- ---

Можно ли использовать KV-Cache для ускорения обучения LLM?
Краткий ответ - нет.
А более подробно, почему это так мы разберем в следующем видео. -->

---
---


Квиз:

* Если рассмотреть асиимптотику по вычислениям для генерации с KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(1)
- O(n)
- O(n^2)
- O(n^3)

Правильный ответ - `O(n^2)`, потому что если для каждого токена требуется `O(n)` вычислений, то для генерации `n` токенов, потребуется `O(n^2)`.

* Если рассмотреть асиимптотику по вычислениям для генерации БЕЗ KV-Cache не одного токена, а последовательности `n` токенов, какая будет сложность?
- O(n)
- O(n^2)
- O(n^3)
- O(n^4)

Правильный ответ - `O(n^3)`, потому что если для каждого токена требуется `O(n^2)` вычислений, то для генерации `n` токенов, потребуется `O(n^3)`.

