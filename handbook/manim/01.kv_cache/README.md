
Настало время разобраться, что такое KV-Cache?

Но сначала вспомним, как работает генерация в трансформерах.

---

Tрансформер - это авторегрессионная модель.
1) За один шаг - генерируем один новый токен
2) Этот новый токен зависит от всех предыдущих

$$ x_t​∼P_{\theta}​(x_t​ \mid x_1, x_2, ..., x_{t-1}​) $$

---

Как бы выглядела генерация без KV-Cache?

На каждом шаге перевычисляются эмбэддинги для вообще всех
токенов. Поэтому количество вычислений
растет **квадратично** с длинной последовательности.

---

При генерации с KV-Cache мы переиспользуем эмбэддинги, вычесленные на предыдущем шаге.
Это требует дополнительной памяти, но зато кол-во вычислений растет **линейно**.

---

Итого получаем трейдофф: память / вычисления.
Квадратичные вычисления - это боль, поэтому
на практике для генерации всегда используется KV-Cache.

---

Кокретный пример:

8B моделька на для 100k токенов требует 13Gb KV-Cache.

Это много, зато скорость генерации выше на 2-3 порядка.

Поэтому KV-Cache - бро

---
---


Квиз:

100k токенов - сколько это в страницах A4 12pt, однострочный интервал?

- 1 страница
- 10 страниц
- 100 страниц
- 1000 страниц