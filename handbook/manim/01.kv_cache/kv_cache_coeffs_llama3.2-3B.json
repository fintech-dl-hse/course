{
  "formula": "size_bytes = a * n_tokens + b",
  "a_bytes_per_token": 114688,
  "b_bytes": 0,
  "meta": {
    "model": "unsloth/Llama-3.2-3B",
    "dtype": "float16",
    "hidden_size": 3072,
    "num_layers": 28,
    "num_attention_heads": 24,
    "num_key_value_heads": 8,
    "head_dim": 128,
    "counts": [
      10000,
      100000,
      1000000
    ]
  }
}