Agentic AI

Степени автономии

- Наименее автономный: заранее определенная последовательность шагов (напиши запрос для поиска, форсим поиск, и т д)
- Более автономный: сам опрдееляет, когда нужно использовать поиск, а когда - нет, может быть зациклена, сама определяет, когда надо остановиться
- Самый автономный:  LLM создает тулзы налету, которыми может пользоваться. 

Чем больше автономности, тем меньше контроля, сложнее валидировать и больше непредсказуемости.

---

Профиты агентности:

- Качество
HumanEval - gpt3.5 хуже чем gpt4, но агентские системы поверх базовых моделей работают одинаково хорошо, почти все выбивают
- Скорость
Можно распараллелить часть работы (например, поиск и суммаризация)


Декомпозиция

Надо разбивать задачки на шаги - это более предсказуемо. Разобрать пример

Оценка агентов

- Сначала генерим кучу ответов и смотрим глазами, что не нравится. (Пример, что агент упоминает конкурентов)

Объективная оценка: 
Для найденных проблем - обновляем промт, выстраиваем объективную метрику (тесты), провреям, что подстрокой можно найти название конкурента в ответе

Субъективная оценка
Если ошибку нельзя проверить кодом, надо использовать LLM as a judge  ()

И еще есть разделение оценок: end-to-end / оценка по компонентам

Паттерны:
- Reflection: агент сам может получить фидбэк от среды, что что-то не работает и исправить или может быть отдельный агент, который критикует и дает фидбэк текущему агенту

- Использование тулзов: поиск, исполнение кода, калькулятор и т д

- Planning - декомпоизровать одну задачу на несколько, исполнить по шагам

- Multi-agent: коллаборация агентов

------

Reflection

Итеративно просим улучшить текст.
- Можно драфт генерить простой моделькой, более уже следующие шаги
- Можно добавлять компилятор кода, чтобы получить фидбэк от среды
- Можно получить фидбэк от критика?

-- 

Есть работа (self-refine), в которх показали, что рефлексия стабильно на большом кол-ве задач улучшает результаты бенчей. 

--

Лучше всего явно прописать критерии, что нужно улучшить и что нужно проверить. 

---


Objective Evals

Берем задачу, сами пишем запрос и объективный ответ (заведомо правильный). Замеряем с рефлексией и без рефлексии. При каждой модификации промта перезапускаем евал, чтобы оценить качество.

LLM As-A-Judge


Проблемы, если просим сравнить два результата:
- position bias
- bad answers

Как надо:
- вместо одной оценки (от 1 до 5), лучше сделать 5 бинарных оценок по фиксированным характеристикам

----

External Feedback

Когда тюн промта уже не помогает, может помочь рефлексия. Но рефлексия более эффективная с обратной связью от среды. Все метрики, которые ты в промте указал, можно оценить или объективно или через LLM AAJ, эту обратную связь можно передать в рефлексию, чтобы LLM улучшила результат. 

---


Tools

Инструменты помогают LLM сделать 
- детерменировано хорошо (компилятор, валидатор html, кулькулятор)
- LLM не умеет (поиск, текущее время и т д )


Для реализации инструмента - это просто ф-я, которая принимает аргумент и возвращает текст 

Имбовая тула - это писать код. Можно за одно сделать фидбэк с проверкой синтаксиса и рефлексией. Но ноужно быть аккуратным с запуском - лучше запускать в докерах, чтобы не сделать случайно rm -rf 

----

Evaluations

Quick and dirty is ok for start

Per component analysis

Если агент работает в многостадийном режиме, проблема может быть на любом этапе, даже в инструментах.

* Сохраняем трейсы работы агентской системы (и время работы каждого этапа)
* Иногда смотрим на результат работы каждого шага
* Аналиизируем ошибки на каждом шаге, чтобы понять, где именно происходит больше всего проблем и где корень проблемы?

Мораль: длинные агентские флоу - это боль, потому что если где-то в начале флоу что-то сломалось, остальные шаги заведомо будут работать с плохими данными.

--

**Non-llm-based component**

* tune hyper paranmeters
* replace component

**llm-based**

* improve prompt (explicit instructions, examples)
* try new model (choose best based on evals)
* decompose steps
* fine-tune model (if none of other helped)

Читайте промты других людей и проектов, чтобы обучиться писать промты)


Observability:
* сохраняем время работы каждого этапа и кол-во токенов (чтобы пересчитать на деньги потом)


Highly Autonomous Agents

Planning
Сначала просим модель описать план. План должен быть в виде списка инструментов и задач, которые нужно выполнить за каждый шаг. 

После рассуждения, можно попросить структурировать план в json, чтобы избежать неопределенности


Planngng with code execution

ReAct?
Работает лучше, чем планирование через json или текст

Multi-agent workflows

Хотя LLM - это по сути одни веса, но промт действительно может решать. И разбивание задачи на подзадачи часто помогает решить проблему.

Почему мульти-агенты?
- Разные системные промты
- Для разных агентов может требоваться разный набор тулов

Коммуникация мультиагентных систем:

В линейной коммуникации, детерменированном (запрограммированном) флоу все просто. 

Иерархичная коммуникация
Агент-менеждер:
Использует других агентов, как инструменты. В контекст получает только (что?) результат работы других агентов и ставит им задачки, определяет, какого агента дальше позвать. Это называется иерархия агентов. Иерархия может быть более глубокой.

Но вызывает других агентов не как код, а через json'ы. Как код, наверное, не получится и нет смысла.

Пример глубокой иерархии:

Агент-Менеждер
1. Исследователь
1.1 Факт-чекер
1.2 Исследователь интернета
2. Графический дизайнер
3. Писатель
3.1 Проверятель читат

All-to-all
Когда каждый агент коммуницирует с каждым другим. Редко встречается, сложно управлять, скорее всего.