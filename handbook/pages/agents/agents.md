# Agentic AI

Заметки с [курса Andrew Ng по агентским системам](https://learn.deeplearning.ai/courses/agentic-ai/).

### Степени автономии

- Наименее автономный: заранее определенная последовательность шагов (сформировать запрос для поиска, выполнить поиск и т. д.)
- Более автономный: сам определяет, когда использовать поиск и когда остановиться; возможны циклы
- Самый автономный: LLM создает инструменты на лету и использует их

Чем выше автономность, тем меньше контроля, сложнее валидация и больше непредсказуемости.

### Преимущества агентности

- Качество: HumanEval — gpt-3.5 хуже, чем gpt-4, но агентские системы поверх базовых моделей показывают схожие результаты и часто достигают высоких показателей
- Скорость: можно распараллелить часть работы (например, поиск и суммаризацию)

### Декомпозиция

Задачи стоит разбивать на шаги — так предсказуемее и стабильнее. Разобрать пример.

### Оценка агентов (Объективная оценка)

- Сначала сгенерировать множество ответов и вручную выявить проблемы (например, агент упоминает конкурентов)
- Далее для найденных проблем обновлять промт и выстраивать объективные метрики (тесты), проверяя, например, что подстрокой можно найти название конкурента в ответе
- Также выделять уровни оценки: end-to-end и по компонентам

#### LLM-as-a-Judge (Субъективная оценка)

Если ошибку нельзя проверить кодом, использовать LLM как судью.

- Проблемы при сравнении двух результатов: position bias, плохие ответы
- Лучше вместо одной общей оценки (1–5) использовать несколько бинарных оценок по фиксированным характеристикам

### Паттерны

- Reflection: агент получает обратную связь от среды и исправляется сам или с помощью отдельного агента-критика
- Использование инструментов: поиск, исполнение кода, калькулятор и т. д.
- Planning: декомпозировать задачу на несколько шагов и исполнять последовательно
- Multi-agent: коллаборация агентов

### Reflection

Итеративное улучшение результата.
- Драфт можно генерировать простой моделью, а дальнейшие шаги — более сильной
- Добавление компилятора кода дает обратную связь от среды
- Можно получить обратную связь от критика

Есть работа (self-refine), где показано, что рефлексия стабильно на большом числе задач улучшает результаты бенчмарков.

{% note tip %}
Лучше явно сформулировать критерии: что улучшать и что проверять.
{% endnote %}

### Objective Evals

Берем задачу, заранее формируем запрос и объективный правильный ответ. Замеряем качество с рефлексией и без нее. При каждой модификации промта перезапускаем оценку, чтобы отследить изменения качества.

### External Feedback

Когда тюнинг промта не помогает, может выручить рефлексия — особенно эффективна она с обратной связью от среды. Метрики из промта можно оценивать либо объективно, либо через LLM-as-a-Judge; эту обратную связь передаем в рефлексию, чтобы LLM улучшила результат.

### Tools

Инструменты помогают LLM:
- Делать детерминированно хорошо (компилятор, валидатор HTML, калькулятор)
- Доставать недоступное LLM (поиск, текущее время и т. д.)

Технически инструмент — это функция, принимающая аргументы и возвращающая текст.

Сильный инструмент — исполнение кода. Можно сразу получать обратную связь (проверка синтаксиса) и запускать рефлексию. Но запуск кода требует аккуратности — лучше исполнять в контейнерах, чтобы исключить риск наподобие rm -rf.

### Evaluations

Quick and dirty — ок для старта.

#### По-компонентный анализ

Если агент работает многостадийно, проблема может возникать на любом этапе, включая инструменты.

- Сохраняем трейсы работы (и время каждого этапа)
- Периодически смотрим на результаты каждого шага
- Анализируем ошибки по шагам, чтобы понять, где чаще всего возникают проблемы и каков их корень

{% note warning %}
Мораль: длинные агентные флоу — это боль. Если в начале что-то сломалось, дальнейшие шаги работают с плохими данными.
{% endnote %}

**Non-llm-based component**

- Tune hyperparameters
- Replace component

**llm-based**

- Improve prompt (explicit instructions, examples)
- Try new model (choose best based on evals)
- Decompose steps
- Fine-tune model (if none of other helped)

Читайте промты других людей и проектов, чтобы учиться писать промты.

### Observability

- Сохраняем время работы каждого этапа и количество токенов (для пересчета стоимости)

### Highly Autonomous Agents

#### Planning

Сначала просим модель описать план: список инструментов и задач по шагам. После рассуждения можно попросить структурировать план в JSON, чтобы избежать неоднозначности.

#### Planning with code execution

ReAct. Работает лучше, чем планирование через JSON или текст.

### Multi-agent workflows

Хотя LLM — это по сути одни веса, промт имеет значение. Разбиение задачи на подзадачи часто помогает.

#### Почему мультиагенты?
- Разные системные промты
- Для разных агентов может требоваться разный набор инструментов

#### Коммуникация мультиагентных систем:

- Линейная коммуникация: детерминированный (запрограммированный) флоу
- Иерархичная коммуникация. Агент-менеджер:
  Использует других агентов как инструменты. В контекст получает только результат их работы, ставит задачи и определяет, какого агента вызывать дальше. Это иерархия агентов, она может быть глубокой.
- All-to-all. Когда каждый агент коммуницирует с каждым другим. Редко встречается и, вероятно, сложно управляется.


Вызывает других агентов не как код, а через JSON.

Пример глубокой иерархии:

Агент-Менеджер
1. Исследователь
1.1 Факт-чекер
1.2 Исследователь интернета
2. Графический дизайнер
3. Писатель
3.1 Проверятель читат

