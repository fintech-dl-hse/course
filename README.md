# Материалы курса Глубинное обучение

* Прогон курса в [2023 году](https://github.com/fintech-dl-hse/course/tree/2023)
* Прогон курса в [2022 году](https://github.com/fintech-dl-hse/course/tree/2022)
* [Разборы семинаров 2022 года](https://youtube.com/playlist?list=PLCNrwCOlxMPxUnJNtthxVdL3eYAj0Lp1Q)

### Планируется

* ~15 лекций и семинаров
* ~5 домашек

Материалы будут выкладываться в течение курса.

**Обратная связь.** пожалуйста, заполняйте формы обратной связи. Желательно заполнять открытые вопросы --- в них можете высказать все, что угодно)

**Сложность курса.** Понятно, что на курсе есть люди с разными бэкграундами. Если вам что-то не понятно, не стесняйтесь задавать вопрос.

## Как задавать вопросы?

* не стесняться, не бывает глупых вопросов
* в общий чатик --- так есть вероятность, что вам может помочь кто-то из одногруппников быстрее, чем преподаватель и на преподавателей меньше нагрузка
* на лекциях или семинарах
* в личку


## Как построены семинары?

* Работа над ошибками по обратной связи. Если че-то не нравится на семах, пишите в формочки обратной связи, будем думать
* Повторение, что прошли на прошлом семе
* Новый материал
* Блиц по новому материалу
* Обзор домашки, если она есть

## Домашки

| Домашка                  | Количество баллов | Дедлайн                           |
| ------------------------ | ----------------- | --------------------------------- |
| weight-init              | 1                 |      12.02 23:59 (2 недели)             |
| activations              | 1                 |      12.02 23:59 (2 недели)             |
| optimization             | 2                 |      04.03 23:59 (до конца блока CV)    |
| batchnorm                | 1                 |      04.03 23:59 (до конца блока CV)    |
| dropout                  | 1                 |      04.03 23:59 (до конца блока CV)    |
| pytorch-basics           | 2                 |      04.03 23:59 (до конца блока CV)    |
| letters                  | 4                 |      20.05 23:59 (до последней лекции)  |
| hw-rnn-attention         | 2                 |      15.04 23:59 (2 недели)             |
| hw-transformer-attention | 2                 |      22.04 23:59 (2 недели)             |
| vae                      | 2                 |      10.06 23:59 (3 недели)             |
| diffusion                | 2                 |      10.06 23:59 (3 недели)             |

Максимум можно получить 20 баллов за все домашки.

**Списывание (!):** За списывание зануляются все работы. Если используете код из открытых источников, пожалуйста указывайте ссылки.

**Сдача:** Домашки будут сдаваться в github classroom.

В домашках настроено автоматическое оценивание. Временем сдачи домашки будет считаться
время, когда прошли пайплайны (то есть не время коммита, а время пуша + время на прогон тестов).
За попытки хакнуть, поломать или обмануть автогрейдер балл за домашку зануляется.

За неоптимальные решения или за неправильные решения баллы могут снижаться.

**Штрафы:** За просрочку за каждый день будет сниматься по `10%` от оценки, но суммарно штраф не может быть более `30%`. Жестких дедлайнов нет. Если сдаешь через 3 дня домашку, штраф `30%`. Если сдаешь через месяц, штраф тоже `30%`

## Формула

Оценка за курс складывается из нормированной суммы баллов за выполненные домашки.

$$ O_{hse} = \frac{1}{20} \sum_{i} O_{hw_i} $$

## Примерный план

| Дата  | Лекция                                                           | Домашка                          |
|-------|------------------------------------------------------------------|----------------------------------|
| 15.01 | Введение в глубинное обучение.                                   |                                  |
| 22.01 | Обучение нейросетей. Алгоритм обратного распространения ошибки.  |                                  |
| 29.01 | Функции активации. Задачи и функции потерь. Инициализация весов. | weight-init, activations         |
| 05.02 | Оптимизация. Регуляризация.                                      | optimization, dropout            |
| 12.02 | Сверточные сети.                                                 | pytorch-basics, batchnorm        |
| 19.02 | -                                                                |                                  |
| 26.02 | Задачи Computer Vision (гостевая лекция)                         | letters                          |
| 04.03 | NLP, Word2vec.                                                   |                                  |
| 11.03 | Рекуррентные сети.                                               |                                  |
| 18.03 | -                                                                |                                  |
| 25.03 | -  _каникулы_                                                    |                                  |
| 01.04 | Attention, Transformers.                                         | hw-transformer-attention, hw-rnn-attention         |
| 08.04 | Pretrained transformers in NLP.                                  |                                  |
| 15.04 | Из NLP в CV: Vision Transformers. Contrastive learning.          |                                  |
| 22.04 | Гостевая лекция?                                                 |                                  |
| 29.04 | Генеративные модели: Авторегрессионные, GAN.                     |                                  |
| 06.05 | -  _майские_                                                     |                                  |
| 13.05 | Генеративные модели: VAE, Diffusion.                             | vae, diffusion                   |
| 20.05 | Дополнительные темы                                              |                                  |
| 27.05 |                                                                  |                                  |


## Литература

* *Глубокое обучение. Погружение в мир нейронных сетей.* С. Николенко, А. Кадурин, Е. Архангельская.

* *Глубокое обучение.* Я. Гудфеллоу, Й. Бенджио, А. Курвилль.
Есть английская версия: [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/).

* *Understanding deep learning.* S. Prince.
[https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/).

* *Dive into Deep Learning.* A. Zhang, Z. Lipton, M. Li, A.
Smola.
[https://d2l.ai/](https://d2l.ai/).


## Полезные материалы

* [Материалы с курса бакалавриата](https://github.com/aosokin/dl_cshse_ami/tree/master/2021-fall)
* [Курс ШАДа](https://github.com/yandexdataschool/Practical_DL)
* [Курс Стендфорда про CV with DL](https://cs231n.github.io/)
* [Курс Стендфорда про NLP with DL](http://web.stanford.edu/class/cs224n/) --- один из лучших курсов про NLP
* [DLSchool](https://www.dlschool.org/) --- много классных домашек и проект в конце курса, там есть и про NLP, и про CV, на некоторых прогонах затрагивали и работу со звуком (на ютубе можно найти записи прогонов прошлых лет, на степики старые прогоны курса тоже можно найти)
* [Вводный курс Семена Козлова](http://dlcourse.ai/)
* [Lena Voita](https://lena-voita.github.io) --- классный блог и курс по NLP
* [Jay Alammar](https://jalammar.github.io/), [Sebastian Ruder](https://ruder.io/) --- еще популярные блоги про NLP
* [distill.pub](https://distill.pub/) --- журнал с красивыми визуализациями
* [paperswithcode](https://paperswithcode.com/) --- сравнение разных архетектур/задач
* [How To Read Papers](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf)

**Гуглите:** эти и многие другие материалы легко находятся, если вы пытаетесь разобраться в какой-то теме.

**Читайте документацию:** в [Pytorch Docs](https://pytorch.org/docs/stable/index.html), [Pytorch Tutorials](https://pytorch.org/tutorials/) можно найти и описание методов, и формулы, и ссылки на статьи.

**Читайте статьи:** большинство концептов, которые мы проходим в этом курсе, были опубликованы в статьях, которые доступны на [arxiv](https://arxiv.org/). Где следить за самыми современными методами: конференции NeurIPS, ICML, ICLR, CVPR, ACL, блоги крупных компаний [Google AI](https://ai.googleblog.com/), [DeepMind](https://deepmind.com/blog), [OpenAI](https://openai.com/blog/), [Meta AI](https://ai.facebook.com/blog/).
