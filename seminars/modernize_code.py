#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞ –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–º –Ω–æ—É—Ç–±—É–∫–µ.
–î–æ–±–∞–≤–ª—è–µ—Ç type hints, docstrings –∏ —É–ª—É—á—à–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.
"""

import json
from pathlib import Path
from typing import Dict

def load_notebook(path: Path) -> Dict:
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_notebook(notebook: Dict, path: Path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, ensure_ascii=False, indent=1)
    print(f"‚úÖ Notebook saved to: {path}")

# Modernized MLP class with type hints and docstrings
MODERN_MLP_CODE = """from typing import Optional, Callable
import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    \"\"\"
    Multi-Layer Perceptron –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
        input (2) -> hidden (100) -> hidden (100) -> output (1)

    Args:
        activation_cls: –ö–ª–∞—Å—Å —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é nn.Sigmoid)
    \"\"\"

    def __init__(self, activation_cls: Callable = nn.Sigmoid):
        super().__init__()

        # –°–ª–æ–∏ —Å–µ—Ç–∏
        self.input_layer = nn.Linear(2, 100)    # [batch, 2] -> [batch, 100]
        self.hidden_layer = nn.Linear(100, 100)  # [batch, 100] -> [batch, 100]
        self.output_layer = nn.Linear(100, 1)    # [batch, 100] -> [batch, 1]
        self.activation = activation_cls()

    def forward(self, x_coordinates: torch.Tensor) -> torch.Tensor:
        \"\"\"
        Forward pass —á–µ—Ä–µ–∑ —Å–µ—Ç—å.

        Args:
            x_coordinates: –í—Ö–æ–¥–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç–æ—á–µ–∫, shape [batch_size, 2]

        Returns:
            scores: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–∫–æ—Ä—ã, shape [batch_size]
        \"\"\"
        latents = self.activation(self.input_layer(x_coordinates))  # [batch_size, 100]
        latents = self.activation(self.hidden_layer(latents))       # [batch_size, 100]
        scores = self.output_layer(latents)                         # [batch_size, 1]
        scores = scores[:, 0]                                       # [batch_size]

        return scores


# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
model = MLP()
print(model)
print("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:", sum(p.numel() for p in model.parameters()))
"""

# Modernized loss function
MODERN_LOSS_CODE = """def loss(model: nn.Module,
         Xbatch: np.ndarray,
         ybatch: np.ndarray,
         alpha: float = 1e-4) -> tuple[torch.Tensor, torch.Tensor]:
    \"\"\"
    –í—ã—á–∏—Å–ª—è–µ—Ç SVM-style max-margin loss —Å L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π.

    Args:
        model: –ú–æ–¥–µ–ª—å PyTorch
        Xbatch: –ë–∞—Ç—á –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, shape [batch_size, 2]
        ybatch: –ë–∞—Ç—á –º–µ—Ç–æ–∫ (0 –∏–ª–∏ 1), shape [batch_size]
        alpha: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏

    Returns:
        total_loss: –°—É–º–º–∞—Ä–Ω—ã–π loss (data loss + regularization)
        accuracy: –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –±–∞—Ç—á–µ
    \"\"\"
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy -> torch
    Xbatch = torch.tensor(Xbatch, dtype=torch.float32)  # [batch_size, 2]
    ybatch = torch.tensor(ybatch, dtype=torch.float32).unsqueeze(-1)  # [batch_size, 1]

    # Forward pass –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    model_prediction = model.forward(Xbatch)  # [batch_size]

    # SVM max-margin loss: max(0, 1 - y_true * y_pred)
    losses = F.relu(1 - ybatch * model_prediction.unsqueeze(-1))  # [batch_size, 1]
    data_loss = losses.mean()

    # L2 regularization: alpha * sum(w^2)
    reg_loss = alpha * sum((p * p).sum() for p in model.parameters())
    total_loss = data_loss + reg_loss

    # Accuracy
    accuracy = ((ybatch > 0) == (model_prediction.unsqueeze(-1) > 0)).float().mean()

    return total_loss, accuracy

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
total_loss, acc = loss(model, X, y)
print(f"Loss: {total_loss:.4f}, Accuracy: {acc:.2%}")
"""

# Modernized train function
MODERN_TRAIN_CODE = """def train(model: nn.Module,
          learning_rate: float = 0.1,
          num_steps: int = 300):
    \"\"\"
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É—è —Ä—É—á–Ω–æ–π SGD.

    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate)
        num_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    \"\"\"
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    Xbatch, ybatch = make_moons(n_samples=100, noise=0.1, random_state=1)
    ybatch = ybatch * 2 - 1  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–∫–∏: 0,1 -> -1,1

    for k in range(num_steps):
        # 1. –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞
        model.zero_grad()

        # 2. Forward pass: –≤—ã—á–∏—Å–ª—è–µ–º loss
        total_loss, acc = loss(model, Xbatch, ybatch)

        # 3. Backward pass: –≤—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        # –ó–¥–µ—Å—å –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç "–º–∞–≥–∏—è" - PyTorch –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç
        # –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –∏—Å–ø–æ–ª—å–∑—É—è backpropagation!
        total_loss.backward()

        # 4. –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫: w_new = w_old - lr * gradient
        with torch.no_grad():  # –û—Ç–∫–ª—é—á–∞–µ–º autograd –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤
            for p in model.parameters():
                p.data = p.data - learning_rate * p.grad

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if k % 50 == 0:
            print(f"Step {k:3d}: loss = {total_loss.item():.4f}, accuracy = {acc.item():.2%}")


# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
model = MLP()
train(model, learning_rate=0.1)
"""

def modernize_notebook():
    """Modernize code in the merged notebook."""

    notebook_path = Path("01_seminar_mlp_autograd.ipynb")

    print(f"üìñ Loading {notebook_path}...")
    nb = load_notebook(notebook_path)

    print("üîß Modernizing code...")

    # Find and replace MLP class definition
    for i, cell in enumerate(nb['cells']):
        if cell['cell_type'] == 'code':
            source = ''.join(cell.get('source', []))

            # Replace MLP class
            if 'class MLP(nn.Module):' in source and 'def __init__(self' in source:
                print(f"  Updating MLP class at cell {i}")
                cell['source'] = MODERN_MLP_CODE.split('\n')

            # Replace loss function
            elif 'def loss(model, Xbatch, ybatch):' in source:
                print(f"  Updating loss function at cell {i}")
                cell['source'] = MODERN_LOSS_CODE.split('\n')

            # Replace train function
            elif 'def train(model, learning_rate' in source:
                print(f"  Updating train function at cell {i}")
                cell['source'] = MODERN_TRAIN_CODE.split('\n')

    # Save
    print(f"\nüíæ Saving modernized notebook...")
    save_notebook(nb, notebook_path)

    print("\n‚ú® Done! Code has been modernized.")
    print("\nüìù Changes made:")
    print("   - Added type hints to MLP class, loss, and train functions")
    print("   - Added comprehensive docstrings")
    print("   - Improved code comments")
    print("   - Better variable names and structure")

if __name__ == "__main__":
    modernize_notebook()
