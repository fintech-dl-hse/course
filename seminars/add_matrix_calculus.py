#!/usr/bin/env python3
"""
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ –æ –º–∞—Ç—Ä–∏—á–Ω–æ–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ –Ω–æ—É—Ç–±—É–∫.
"""

import json
from pathlib import Path

def load_notebook(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_notebook(notebook, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, ensure_ascii=False, indent=1)

def create_markdown_cell(text):
    lines = text.strip().split('\n')
    source = [line + '\n' for line in lines[:-1]]
    if lines:
        source.append(lines[-1])
    return {
        "cell_type": "markdown",
        "source": source,
        "metadata": {}
    }

def create_code_cell(code):
    lines = code.strip().split('\n')
    source = [line + '\n' for line in lines[:-1]]
    if lines:
        source.append(lines[-1])
    return {
        "cell_type": "code",
        "execution_count": None,
        "source": source,
        "outputs": [],
        "metadata": {}
    }

def add_matrix_calculus_section():
    """Add matrix calculus section to the notebook."""

    print("üìñ Loading notebook...")
    nb = load_notebook("01_seminar_mlp_autograd.ipynb")

    print("üîç Finding insertion point...")
    # Find the cell after "Backpropagation + Chain rule"
    insertion_index = None
    for i, cell in enumerate(nb['cells']):
        if cell['cell_type'] == 'markdown':
            source_text = ''.join(cell.get('source', []))
            if 'Backpropagation + Chain rule' in source_text:
                # Find the next cell after the chain rule explanation
                # We'll insert after all related cells
                insertion_index = i + 1
                # Skip any immediately following cells that are part of the same section
                while insertion_index < len(nb['cells']):
                    next_cell = nb['cells'][insertion_index]
                    if next_cell['cell_type'] == 'markdown':
                        next_text = ''.join(next_cell.get('source', []))
                        # If we hit a new major section, stop
                        if next_text.strip().startswith('# '):
                            break
                    insertion_index += 1
                break

    if insertion_index is None:
        print("‚ùå Could not find insertion point!")
        return

    print(f"‚úÖ Found insertion point at cell {insertion_index}")

    # Create new cells for matrix calculus section
    new_cells = []

    # Main section header
    new_cells.append(create_markdown_cell("""---

# –ú–∞—Ç—Ä–∏—á–Ω–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ

–ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏ –º—ã –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∏–º–µ–µ–º –¥–µ–ª–æ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏. –ß—Ç–æ–±—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è.

## –û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è:

- **–°–∫–∞–ª—è—Ä—ã**: $a, b, c$ (–æ–±—ã—á–Ω—ã–µ —á–∏—Å–ª–∞)
- **–í–µ–∫—Ç–æ—Ä—ã**: $\\mathbf{x}, \\mathbf{y}, \\mathbf{w}$ (–∂–∏—Ä–Ω—ã–µ —Å—Ç—Ä–æ—á–Ω—ã–µ –±—É–∫–≤—ã)
  - $\\mathbf{x} \\in \\mathbb{R}^n$ ‚Äî –≤–µ–∫—Ç–æ—Ä-—Å—Ç–æ–ª–±–µ—Ü —Ä–∞–∑–º–µ—Ä–∞ $n$
- **–ú–∞—Ç—Ä–∏—Ü—ã**: $W, X, A$ (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã)
  - $W \\in \\mathbb{R}^{m \\times n}$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ $m \\times n$
- **–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ**: $\\mathbf{x}^T, W^T$"""))

    # Types of derivatives
    new_cells.append(create_markdown_cell("""## –¢–∏–ø—ã –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö

–í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –ø–æ —á–µ–º—É –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º, –ø–æ–ª—É—á–∞–µ–º —Ä–∞–∑–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã:

| –ß–∏—Å–ª–∏—Ç–µ–ª—å | –ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å | –†–µ–∑—É–ª—å—Ç–∞—Ç | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å | –ù–∞–∑–≤–∞–Ω–∏–µ |
|-----------|-------------|-----------|-------------|----------|
| –°–∫–∞–ª—è—Ä $y$ | –í–µ–∫—Ç–æ—Ä $\\mathbf{x} \\in \\mathbb{R}^n$ | –í–µ–∫—Ç–æ—Ä | $n \\times 1$ | –ì—Ä–∞–¥–∏–µ–Ω—Ç |
| –í–µ–∫—Ç–æ—Ä $\\mathbf{y} \\in \\mathbb{R}^m$ | –°–∫–∞–ª—è—Ä $x$ | –í–µ–∫—Ç–æ—Ä | $m \\times 1$ | –¢–∞–Ω–≥–µ–Ω—Å |
| –í–µ–∫—Ç–æ—Ä $\\mathbf{y} \\in \\mathbb{R}^m$ | –í–µ–∫—Ç–æ—Ä $\\mathbf{x} \\in \\mathbb{R}^n$ | –ú–∞—Ç—Ä–∏—Ü–∞ | $m \\times n$ | –Ø–∫–æ–±–∏–∞–Ω |
| –°–∫–∞–ª—è—Ä $y$ | –ú–∞—Ç—Ä–∏—Ü–∞ $W \\in \\mathbb{R}^{m \\times n}$ | –ú–∞—Ç—Ä–∏—Ü–∞ | $m \\times n$ | –ì—Ä–∞–¥–∏–µ–Ω—Ç |

**–í–∞–∂–Ω–æ**: –í –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö —á–∞—â–µ –≤—Å–µ–≥–æ –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è **—Å–∫–∞–ª—è—Ä–∞** (—Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å) –ø–æ **–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º** (–≤–µ–∫—Ç–æ—Ä–∞–º –∏–ª–∏ –º–∞—Ç—Ä–∏—Ü–∞–º)."""))

    # Basic rules
    new_cells.append(create_markdown_cell("""## –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –º–∞—Ç—Ä–∏—á–Ω–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è

### 1. –õ–∏–Ω–µ–π–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏

**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –ª–∏–Ω–µ–π–Ω–æ–π —Ñ–æ—Ä–º—ã:**
$$\\frac{\\partial (\\mathbf{a}^T \\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{a}$$

–≥–¥–µ $\\mathbf{a}$ ‚Äî –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä, $\\mathbf{x}$ ‚Äî –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è.

**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ–æ—Ä–º—ã:**
$$\\frac{\\partial (\\mathbf{x}^T W \\mathbf{x})}{\\partial \\mathbf{x}} = (W + W^T) \\mathbf{x}$$

–ï—Å–ª–∏ $W$ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è ($W = W^T$), —Ç–æ:
$$\\frac{\\partial (\\mathbf{x}^T W \\mathbf{x})}{\\partial \\mathbf{x}} = 2W\\mathbf{x}$$

**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –Ω–æ—Ä–º—ã:**
$$\\frac{\\partial \\|\\mathbf{x}\\|^2}{\\partial \\mathbf{x}} = \\frac{\\partial (\\mathbf{x}^T \\mathbf{x})}{\\partial \\mathbf{x}} = 2\\mathbf{x}$$"""))

    new_cells.append(create_markdown_cell("""### 2. –ú–∞—Ç—Ä–∏—á–Ω–æ-–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ

**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –ø–æ –≤–µ–∫—Ç–æ—Ä—É:**
$$\\frac{\\partial (W\\mathbf{x})}{\\partial \\mathbf{x}} = W^T$$

**–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –ø–æ –º–∞—Ç—Ä–∏—Ü–µ:**
$$\\frac{\\partial (\\mathbf{a}^T W \\mathbf{x})}{\\partial W} = \\mathbf{a} \\mathbf{x}^T$$

–≥–¥–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ —Ä–∞–∑–º–µ—Ä–∞ $m \\times n$ (–≤–Ω–µ—à–Ω–µ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤).

### 3. Chain Rule –¥–ª—è –º–∞—Ç—Ä–∏—Ü

–ï—Å–ª–∏ $y = f(\\mathbf{z})$ –∏ $\\mathbf{z} = g(\\mathbf{x})$, —Ç–æ:

$$\\frac{\\partial y}{\\partial \\mathbf{x}} = \\frac{\\partial y}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}$$

–≥–¥–µ $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}}$ ‚Äî —ç—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ –Ø–∫–æ–±–∏ —Ä–∞–∑–º–µ—Ä–∞ $|\\mathbf{z}| \\times |\\mathbf{x}|$."""))

    # Examples for neural networks
    new_cells.append(create_markdown_cell("""## –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π

### –ü—Ä–∏–º–µ—Ä 1: –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π

–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π: $\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}$

**–î–∞–Ω–æ**: –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ—Ç–µ—Ä—å –ø–æ –≤—ã—Ö–æ–¥—É $\\frac{\\partial L}{\\partial \\mathbf{z}}$

**–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏**:
- $\\frac{\\partial L}{\\partial W}$ ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤–µ—Å–∞–º
- $\\frac{\\partial L}{\\partial \\mathbf{x}}$ ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Ö–æ–¥—É (–¥–ª—è backprop –¥–∞–ª—å—à–µ)
- $\\frac{\\partial L}{\\partial \\mathbf{b}}$ ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ bias

**–†–µ—à–µ–Ω–∏–µ**:

1. –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤–µ—Å–∞–º:
$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\mathbf{z}} \\cdot \\mathbf{x}^T$$

2. –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Ö–æ–¥—É:
$$\\frac{\\partial L}{\\partial \\mathbf{x}} = W^T \\cdot \\frac{\\partial L}{\\partial \\mathbf{z}}$$

3. –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ bias:
$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{z}}$$"""))

    # Code example
    new_cells.append(create_code_cell("""# –ü—Ä–∏–º–µ—Ä: –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è –≤ PyTorch
import torch

# –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ
x = torch.randn(5, requires_grad=True)  # –≤—Ö–æ–¥: –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ 5
W = torch.randn(3, 5, requires_grad=True)  # –≤–µ—Å–∞: –º–∞—Ç—Ä–∏—Ü–∞ 3x5
b = torch.randn(3, requires_grad=True)  # bias: –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ 3

# Forward pass
z = W @ x + b  # z = Wx + b
loss = z.sum()  # –ø—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (—Å—É–º–º–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤)

# Backward pass
loss.backward()

print("–ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ W:")
print(f"  Shape: {W.grad.shape}")
print(f"  –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫: dL/dz * x^T")
print()
print("–ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ x:")
print(f"  Shape: {x.grad.shape}")
print(f"  –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫: W^T * dL/dz")
print()
print("–ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ b:")
print(f"  Shape: {b.grad.shape}")
print(f"  –†–∞–≤–µ–Ω dL/dz")"""))

    # Example 2: MSE loss
    new_cells.append(create_markdown_cell("""### –ü—Ä–∏–º–µ—Ä 2: Mean Squared Error (MSE)

–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: $L = \\frac{1}{n} \\|\\mathbf{y}_{pred} - \\mathbf{y}_{true}\\|^2 = \\frac{1}{n} \\sum_{i=1}^n (y_{pred}^{(i)} - y_{true}^{(i)})^2$

**–ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º:**
$$\\frac{\\partial L}{\\partial \\mathbf{y}_{pred}} = \\frac{2}{n}(\\mathbf{y}_{pred} - \\mathbf{y}_{true})$$

–≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –≤ —Å—Ç–æ—Ä–æ–Ω—É —É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ—à–∏–±–∫–∏, –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º—ã –¥–≤–∏–∂–µ–º—Å—è –≤ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É (gradient descent)."""))

    new_cells.append(create_code_cell("""# –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç MSE
y_pred = torch.randn(10, requires_grad=True)
y_true = torch.randn(10)

# MSE loss
loss = ((y_pred - y_true) ** 2).mean()
loss.backward()

print("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:", y_pred.data[:5])
print("–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:", y_true[:5])
print("–ì—Ä–∞–¥–∏–µ–Ω—Ç:", y_pred.grad[:5])
print()
print("–§–æ—Ä–º—É–ª–∞: dL/dy_pred = 2/n * (y_pred - y_true)")
manual_grad = 2 * (y_pred.data - y_true) / len(y_pred)
print("–†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç:", manual_grad[:5])
print("–°–æ–≤–ø–∞–¥–∞–µ—Ç —Å PyTorch:", torch.allclose(y_pred.grad, manual_grad))"""))

    # Summary
    new_cells.append(create_markdown_cell("""## –ó–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ?

1. **–ü–æ–Ω–∏–º–∞–Ω–∏–µ backpropagation**: –í—Å–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –ø–æ —ç—Ç–∏–º –ø—Ä–∞–≤–∏–ª–∞–º
2. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: –ó–Ω–∞–Ω–∏–µ —Ñ–æ—Ä–º—É–ª –ø–æ–º–æ–≥–∞–µ—Ç –ø–∏—Å–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∫–æ–¥
3. **–û—Ç–ª–∞–¥–∫–∞**: –ú–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤—Ä—É—á–Ω—É—é
4. **–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–∏—Ö —Å–ª–æ–µ–≤**: –ü—Ä–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ custom layers –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å, –∫–∞–∫ –≤—ã—á–∏—Å–ª—è—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

## –ü–æ–ª–µ–∑–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/) ‚Äî –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ç—É—Ç–æ—Ä–∏–∞–ª
- [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) ‚Äî —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Ñ–æ—Ä–º—É–ª
- [CS231n: Backpropagation](https://cs231n.github.io/optimization-2/) ‚Äî –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ backprop —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
- [PyTorch Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) ‚Äî –∫–∞–∫ PyTorch –≤—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã"""))

    # Insert new cells
    print(f"üìù Inserting {len(new_cells)} new cells...")
    nb['cells'] = nb['cells'][:insertion_index] + new_cells + nb['cells'][insertion_index:]

    # Save
    print(f"üíæ Saving updated notebook ({len(nb['cells'])} cells)...")
    save_notebook(nb, "01_seminar_mlp_autograd.ipynb")

    print("‚úÖ Done!")
    print(f"\nüìä Statistics:")
    print(f"   Total cells: {len(nb['cells'])}")
    print(f"   Added cells: {len(new_cells)}")
    print(f"   Code cells: {len([c for c in nb['cells'] if c['cell_type'] == 'code'])}")
    print(f"   Markdown cells: {len([c for c in nb['cells'] if c['cell_type'] == 'markdown'])}")

if __name__ == "__main__":
    add_matrix_calculus_section()
