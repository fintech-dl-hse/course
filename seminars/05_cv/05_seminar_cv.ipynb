{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/fintech-dl-hse/course/blob/main/seminars/05_cv/05_seminar_cv.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGDOWFu8sIan"
      },
      "source": [
        "# Feedback\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HPsVWhMsMUR"
      },
      "source": [
        "## Курс по PyTorch\n",
        "\n",
        "Что именно? Базовые операции в торче не отличаются от numpy. Основное отличие - это поддержка `autograd` в тензорах и возможность вычисления на `GPU`.\n",
        "\n",
        "Отдельно идет описание параметров нейросетевых моделей -- это мы разобрали на 3 семинаре и во всех домашках\n",
        "\n",
        "Напишите, пожалуйста, в личку более подробно, каких именно вам материалов не хватило?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ID8Tnj31vBe"
      },
      "source": [
        "# Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s107-r7AN59R"
      },
      "source": [
        "### `CrossEntropy` vs `BinaryCrossEntropy`?\n",
        "\n",
        "`BinaryCrossEntropy` может прдесказывать для одного объекта одновременно несколько классов. Классы считаются независимыми.\n",
        "\n",
        "\n",
        "`CrossEntropy` тоже можно применять для сегментации если области разметки не пересекются."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlGCUTsa1woR"
      },
      "source": [
        "### Что такое `transfer learning`, `fine-tuning`?\n",
        "\n",
        "Способ переиспользовать параметры сети, обученной на другой задаче.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbnNu7qOuFRT"
      },
      "source": [
        "\n",
        "### Почему это делают?\n",
        "\n",
        "Сети быстрее дообучить, чем обучить с нуля. Иногда с нуля сложныо обучить, потому что датасет маленький. Если датасет большой, то все равно хорошо --- можно считать, что будет больше данных, меньше вероятность переобучиться.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV5QcBPNuGo2"
      },
      "source": [
        "\n",
        "### В чем разница?\n",
        "\n",
        "В общем-то, `fine-tuning` --- это подмножество `transfer learning`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr-GbQP6uID2"
      },
      "source": [
        "\n",
        "### Как заморозить веса сети?\n",
        "\n",
        "Какие бывают можете предложить стратегии разморозки?\n",
        "\n",
        "```python\n",
        "# замораживаем веса для backbone'а сети\n",
        "for p in resnet.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# размораживаем наш классификатор\n",
        "for p in resnet.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "```\n",
        "\n",
        "\n",
        "https://cs231n.github.io/transfer-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td9gwct3qtIy"
      },
      "source": [
        "## Какие параметры выучивают свертки?\n",
        "\n",
        "Вообще говоря, принято считать, что фильтры выучиват текстуры. И используя это можно обмануть нейросеетвой классификатор, например, если показать ему рыбу с текстурой чешуи как кожа коровы, то классификатор, скорее всего будет считать это коровой.\n",
        "\n",
        "Сами ядра можно визуализировать\n",
        "\n",
        "https://deeplizard.com/learn/video/cNBBNAxC8l4\n",
        "\n",
        "<img src=\"https://deeplizard.com/assets/jpg/c50ac9ac.jpg\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx5pWRxCtPEQ"
      },
      "source": [
        "# Архитектуры сверточных сетей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKT_Nj_w6jVv"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/cv_tasks.png\" width=600 />\n",
        "\n",
        "[**Источник**](https://cs231n.stanford.edu/slides/2024/lecture_9.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtVGlqFT5bN-"
      },
      "source": [
        "# Классификация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUv_KogfuNEt"
      },
      "source": [
        "## ResNet\n",
        "\n",
        "**Ключевая фишка:** skip-connection (residual block)\n",
        "\n",
        "**Цель:** Обучить глубокую сеть. Резидуалы помогают градиентам протекать на глубокие слои."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2giL2emUkMy3"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/resnet.png\" width=600 />\n",
        "\n",
        "**Легенда**:\n",
        "Закрашеные квадраты - это активации. Высота квадрата - пространственные размерности. Ширина - количество каналов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9b5844f2d5db4836aebaf437f7b0f905",
            "02a0aac3347844bca1625c4f79034e4e",
            "7a0de550a1b843f98579aebc615f5674",
            "3cd3215840314ab48aa5995c5fea1752",
            "27362a0f7fc94d029093bc320f780ac5",
            "80bbce4c54d44e82b98f54d8057888fe",
            "70e85f065f24470dbe0c8268ac52675f",
            "849da4e24d5c43a3a94a39e7f1859f9e",
            "686915ccbb4d40acbc1e913deb09429d",
            "73f1476c54ec4b3ab8f0d22cd0560adc",
            "253f30b1039b4addae9058fe73fbfbde"
          ]
        },
        "id": "QLWWxx0HOmc9",
        "outputId": "fd865e5b-a8f7-4565-ee86-bf4d626d2ea6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b5844f2d5db4836aebaf437f7b0f905",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.models import resnet18\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "# todo почему он так называется?\n",
        "resnet = resnet18(pretrained=True) # pretrained=True\n",
        "resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfOf_8_tySs3"
      },
      "outputs": [],
      "source": [
        "resnet.fc = nn.Linear(512, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6IfhpzS1XNf"
      },
      "outputs": [],
      "source": [
        "class ExampleResidualModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        conved = self.model(x)\n",
        "        return self.activation(conved) + x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFCFT2mWuM-V"
      },
      "source": [
        "## EfficientNet\n",
        "\n",
        "* [Paper](https://arxiv.org/abs/1905.11946)\n",
        "* [Implementation](https://github.com/lukemelas/EfficientNet-PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAoDYXbhqWCf"
      },
      "source": [
        "\n",
        "----\n",
        "\n",
        "#### [MnasNet](https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html)\n",
        "\n",
        "**Цель:** создать модель для мобильных телефонов, которая может быстро считаться на телефонах.\n",
        "\n",
        "`MnasNet` запустили для мобилок, получили хорошее кач-во, быстрое вычисление. Строили с помощью Neuaral Architecture Search (NAS) оптимизировали скорость вычисления, задержку (latency).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz6u_bRX0ACp"
      },
      "source": [
        "#### EfficientNet\n",
        "\n",
        "**Цель:** создать модель общего назначения, которая хорошо масштабируется\n",
        "\n",
        "В `EfficientNet` использовали NAS со сложным масштабированием (Compound Scaling) -- искали архитектуру одновременно масштабируя и глубину, и ширину сети. Оптимизировали FLOPS'ы (количество вычислений) и качество модели.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ti8hJMMsm3r"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/efficient_net_metrics.png\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxPuSAPpnds5"
      },
      "source": [
        "\n",
        "### [Swish function](https://en.wikipedia.org/wiki/Swish_function)\n",
        "\n",
        "```\n",
        " f(x) = x · sigmoid(x)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_DyeZCnlyI"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/swish.png\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamBrQdin8Lw"
      },
      "source": [
        "### [Depthwise Convolution](https://paperswithcode.com/method/depthwise-convolution)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tgXyzxzoQPj"
      },
      "source": [
        "\n",
        "Depthwise Convolution is a type of convolution where we apply a single convolutional filter for each input channel. In the regular 2D convolution performed over multiple input channels, the filter is as deep as the input and lets us freely mix channels to generate each element in the output. In contrast, depthwise convolutions keep each channel separate. To summarize the steps, we:\n",
        "\n",
        "1. Split the input and filter into channels.\n",
        "1. We convolve each input with the respective filter.\n",
        "1. We stack the convolved outputs together.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/1_yG6z6ESzsRW-9q5F_neOsg_eaJuoa5.png\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyV8oF1hpQw-"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/depthwise_conv_doc.png\" width=400 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzuMH5nw0EpR"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Conv2d\n",
        "\n",
        "Conv2d(10, 20, groups=10)\n",
        "# 10 x Conv2d(1, 2)\n",
        "\n",
        "\n",
        "Conv2d(30, 20, groups=10)\n",
        "# 10 x Conv2d(3, 2)\n",
        "\n",
        "\n",
        "Conv2d(10, 10, groups=10)\n",
        "# 10 x Conv2d(1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aWUFfrmdBkW",
        "outputId": "40275d3b-aa72-4dcd-dfd5-94d701df49e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([2, 2, 3, 3]), torch.Size([2])]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda x: x.shape, list(nn.Conv2d(4, 2, kernel_size=3, groups=2).parameters())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBouJlQMrMLq"
      },
      "outputs": [],
      "source": [
        "list(map(lambda x: x.shape, list(nn.Conv2d(10, 20, kernel_size=3, groups=10).parameters())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFLEjHQZqL52",
        "outputId": "4e3c3b8a-250a-45a7-ea54-dc1ac45c4ce8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum([ p.numel() for p in nn.Conv2d(10, 10, kernel_size=3, groups=10).parameters()]) # depthwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYBBuJvtndEe",
        "outputId": "97007df8-603a-47c1-d31e-9bb6009ec5d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "910"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum([ p.numel() for p in nn.Conv2d(10, 10, kernel_size=3, groups=1).parameters()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a6a8qo-rX8w",
        "outputId": "343c12d5-0212-45cf-f407-6edcae3f1e69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[torch.Size([10, 10, 3, 3]), torch.Size([10])]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(lambda x: x.shape, list(nn.Conv2d(10, 10, kernel_size=3, groups=1).parameters())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BfaDMM_OxTB",
        "outputId": "c82e5645-7ec7-45fc-fb15-f361dda27fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=9d6dde3b4a5ec8cc74db1222137bf1a8bce98edb5eefb33c4c039e7cc5c3f454\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a6dcae05ad1a46c382b83c3d9fee820f",
            "2fb530f78ad14f84b19599c4837d9ae4",
            "602b2d065b1147e48d42be3d17669f1f",
            "fc41776bb53e4daf9f71793145539e9a",
            "4f2dcf2ff14240aeb5a1bf8120fd84e3",
            "b53c79b28cfd4a1aa414547b5abe2666",
            "fd6fffb4b2e4433bbe7948d4a4075fbd",
            "2f70e90a58a5453b9e51261ade3fc793",
            "7639fafca5d34b3a9a789295ec917d25",
            "4f0f0f6984ad49c4b83ba5a401884478",
            "e0de2c16fa9a48b997b86731585a84e4"
          ]
        },
        "id": "882CkJNLOoGj",
        "outputId": "87341aff-b5ae-4ba6-b769-f8315e0370ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6dcae05ad1a46c382b83c3d9fee820f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/20.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (_conv_stem): Conv2dStaticSamePadding(\n",
              "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "  )\n",
              "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_blocks): ModuleList(\n",
              "    (0): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (1): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (2): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (3): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (4): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (5): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (6): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (7): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (8): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (9): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (10): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (11): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (12): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (13): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (14): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (15): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "  )\n",
              "  (_conv_head): Conv2dStaticSamePadding(\n",
              "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "    (static_padding): Identity()\n",
              "  )\n",
              "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "  (_dropout): Dropout(p=0.2, inplace=False)\n",
              "  (_fc): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  (_swish): MemoryEfficientSwish()\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "efficientnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztiQtmypgcWR"
      },
      "source": [
        "## ConvNext\n",
        "\n",
        "**Цель:** создать модель общего назначения, которая хорошо масштабируется.\n",
        "\n",
        "**Фишка:** Переиспользовать архитектурные решения трансформеров, но не использовать блоки внимания, сохраняя полностью сверточную архитектуру.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCEFOx1LgcGc"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "net = torchvision.models.convnext_base(weights='ConvNeXt_Base_Weights.DEFAULT')\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h5tBmfPg5Yv",
        "outputId": "eef9bbf2-b8aa-4ef5-bc5a-1ddecc8405d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ConvNeXt(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
              "      (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.014285714285714285, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.02857142857142857, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.04285714285714286, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.05714285714285714, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.07142857142857142, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.08571428571428572, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.11428571428571428, mode=row)\n",
              "      )\n",
              "      (3): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.12857142857142856, mode=row)\n",
              "      )\n",
              "      (4): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.14285714285714285, mode=row)\n",
              "      )\n",
              "      (5): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.15714285714285714, mode=row)\n",
              "      )\n",
              "      (6): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.17142857142857143, mode=row)\n",
              "      )\n",
              "      (7): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.18571428571428572, mode=row)\n",
              "      )\n",
              "      (8): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
              "      )\n",
              "      (9): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.21428571428571427, mode=row)\n",
              "      )\n",
              "      (10): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.22857142857142856, mode=row)\n",
              "      )\n",
              "      (11): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.24285714285714285, mode=row)\n",
              "      )\n",
              "      (12): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2571428571428571, mode=row)\n",
              "      )\n",
              "      (13): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2714285714285714, mode=row)\n",
              "      )\n",
              "      (14): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.2857142857142857, mode=row)\n",
              "      )\n",
              "      (15): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.3, mode=row)\n",
              "      )\n",
              "      (16): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.3142857142857143, mode=row)\n",
              "      )\n",
              "      (17): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.32857142857142857, mode=row)\n",
              "      )\n",
              "      (18): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.34285714285714286, mode=row)\n",
              "      )\n",
              "      (19): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.35714285714285715, mode=row)\n",
              "      )\n",
              "      (20): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.37142857142857144, mode=row)\n",
              "      )\n",
              "      (21): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.38571428571428573, mode=row)\n",
              "      )\n",
              "      (22): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4, mode=row)\n",
              "      )\n",
              "      (23): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4142857142857143, mode=row)\n",
              "      )\n",
              "      (24): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.42857142857142855, mode=row)\n",
              "      )\n",
              "      (25): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.44285714285714284, mode=row)\n",
              "      )\n",
              "      (26): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.45714285714285713, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4714285714285714, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.4857142857142857, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (4): GELU(approximate='none')\n",
              "          (5): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jcZU8zTp_IG"
      },
      "source": [
        "```python\n",
        "\n",
        "# https://pytorch.org/vision/0.12/_modules/torchvision/models/convnext.html\n",
        "\n",
        "class CNBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        layer_scale: float,\n",
        "        stochastic_depth_prob: float,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim, bias=True),\n",
        "            Permute([0, 2, 3, 1]),\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(in_features=dim, out_features=4 * dim, bias=True),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(in_features=4 * dim, out_features=dim, bias=True),\n",
        "            Permute([0, 3, 1, 2]),\n",
        "        )\n",
        "        self.layer_scale = nn.Parameter(torch.ones(dim, 1, 1) * layer_scale)\n",
        "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        result = self.layer_scale * self.block(input)\n",
        "        result = self.stochastic_depth(result)\n",
        "        result += input\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBL7hgf2qH3_"
      },
      "source": [
        "```python\n",
        "\n",
        "# https://pytorch.org/vision/main/_modules/torchvision/ops/stochastic_depth.html#StochasticDepth\n",
        "import torch\n",
        "import torch.fx\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from ..utils import _log_api_usage_once\n",
        "\n",
        "def stochastic_depth(input: Tensor, p: float, mode: str, training: bool = True) -> Tensor:\n",
        "    \"\"\"\n",
        "    Implements the Stochastic Depth from `\"Deep Networks with Stochastic Depth\"\n",
        "    <https://arxiv.org/abs/1603.09382>`_ used for randomly dropping residual\n",
        "    branches of residual architectures.\n",
        "\n",
        "    Args:\n",
        "        input (Tensor[N, ...]): The input tensor or arbitrary dimensions with the first one\n",
        "                    being its batch i.e. a batch with ``N`` rows.\n",
        "        p (float): probability of the input to be zeroed.\n",
        "        mode (str): ``\"batch\"`` or ``\"row\"``.\n",
        "                    ``\"batch\"`` randomly zeroes the entire input, ``\"row\"`` zeroes\n",
        "                    randomly selected rows from the batch.\n",
        "        training: apply stochastic depth if is ``True``. Default: ``True``\n",
        "\n",
        "    Returns:\n",
        "        Tensor[N, ...]: The randomly zeroed tensor.\n",
        "    \"\"\"\n",
        "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
        "        _log_api_usage_once(stochastic_depth)\n",
        "    if p < 0.0 or p > 1.0:\n",
        "        raise ValueError(f\"drop probability has to be between 0 and 1, but got {p}\")\n",
        "    if mode not in [\"batch\", \"row\"]:\n",
        "        raise ValueError(f\"mode has to be either 'batch' or 'row', but got {mode}\")\n",
        "    if not training or p == 0.0:\n",
        "        return input\n",
        "\n",
        "    survival_rate = 1.0 - p\n",
        "    if mode == \"row\":\n",
        "        size = [input.shape[0]] + [1] * (input.ndim - 1)\n",
        "    else:\n",
        "        size = [1] * input.ndim\n",
        "    noise = torch.empty(size, dtype=input.dtype, device=input.device)\n",
        "    noise = noise.bernoulli_(survival_rate)\n",
        "    if survival_rate > 0.0:\n",
        "        noise.div_(survival_rate)\n",
        "    return input * noise\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    \"\"\"\n",
        "    See :func:`stochastic_depth`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p: float, mode: str) -> None:\n",
        "        super().__init__()\n",
        "        _log_api_usage_once(self)\n",
        "        self.p = p\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        s = f\"{self.__class__.__name__}(p={self.p}, mode={self.mode})\"\n",
        "        return s\n",
        "\n",
        "\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt44O8gI91rb"
      },
      "outputs": [],
      "source": [
        "# [ bs, 1, 1, 1 ]\n",
        "\n",
        "# [[[ 1, 0, 0, 1 ]]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BzgAHwY5mN_"
      },
      "source": [
        "# Семантическая сегментация\n",
        "\n",
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/sem_seg.png\" width=400 />\n",
        "\n",
        "[Источник](https://thegradient.pub/semantic-segmentation/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5GrcU__-yT3"
      },
      "outputs": [],
      "source": [
        "image = torch rand [ 1, 3, 32, 32 ] # [ bs, channels, w, h ]\n",
        "target = torch tensor ~ [ 1, 32, 32, 5 ] # [ bs, w, h, n_classes ]\n",
        "\n",
        "model = nn.Seq( Conv2d(3, 256, ks=3, padding=1), Conv2d(256, 5) )\n",
        "\n",
        "predictions = model(image) # [ bs, n_classes, 8, 8 ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbrw7LbgSZTf"
      },
      "source": [
        "## Unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4kCSQaeVA9d"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/unet.png\" width=600 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sAoRogNjjTC"
      },
      "source": [
        "#### Давайте сравним Unet и ResNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH4yr9WmSZFL"
      },
      "outputs": [],
      "source": [
        "# https://github.com/milesial/Pytorch-UNet/\n",
        "\n",
        "\"\"\" Parts of the U-Net model \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkl14HXo5r47"
      },
      "source": [
        "# Детекция объектов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rtr-lav5tys"
      },
      "source": [
        "## YOLOv5\n",
        "\n",
        "PS Самая свежая версия YOLO - 11. Но мы не сможем ее подробно разобрать, тк там используется блок внимания, который мы пока не проходили.\n",
        "\n",
        "* **Backbone** - CSPDarknet (**C**ross **S**tage **P**artial Network)\n",
        "* **Neck** - PANet (Path Aggregation Network)\n",
        "* **Head** - BBox Prediction, Classes Prediction\n",
        "* **Postprocessing** - Non-Maximum Suppression\n",
        "\n",
        "---\n",
        "\n",
        "[Статья Datasecrets - История YOLO – самой известной архитектуры компьютерного зрения](https://datasecrets.ru/articles/20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx5vqgI7euTB"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/yolov5.png\" width=400 />\n",
        "\n",
        "[Источник](https://www.researchgate.net/figure/The-network-architecture-of-YOLOv5-1-Backbone-CSPDarknet-for-feature-extraction-2_fig1_358553872)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AiMdiQQ-F3v"
      },
      "outputs": [],
      "source": [
        "!pip install -U ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY23VLniTEX8",
        "outputId": "27d0fbc2-5df2-42e2-c9b4-b6274c5de16d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-24 14:11:35--  http://djl.ai/examples/src/test/resources/dog_bike_car.jpg\n",
            "Resolving djl.ai (djl.ai)... 185.199.111.153, 185.199.108.153, 185.199.110.153, ...\n",
            "Connecting to djl.ai (djl.ai)|185.199.111.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 163759 (160K) [image/jpeg]\n",
            "Saving to: ‘dog_bike_car.jpg’\n",
            "\n",
            "\rdog_bike_car.jpg      0%[                    ]       0  --.-KB/s               \rdog_bike_car.jpg    100%[===================>] 159.92K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-02-24 14:11:35 (8.23 MB/s) - ‘dog_bike_car.jpg’ saved [163759/163759]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://djl.ai/examples/src/test/resources/dog_bike_car.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "90Pj6O2KShJj",
        "outputId": "25856bac-3f3e-4fb3-d578-2132da8ac4af"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Загрузка предобученной модели (YOLOv5s - самая лёгкая версия)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Model config\n",
        "# https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.to(device)\n",
        "\n",
        "# Загрузка изображения\n",
        "image_path = 'dog_bike_car.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Отображаем исходное изображение\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Запуск предсказания\n",
        "results = model(image)\n",
        "\n",
        "# Отображение результатов\n",
        "results.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT9onQ95Yx48",
        "outputId": "187ee2fb-7094-487e-bffd-7f26f3b8bfee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "YOLOv5 <class 'models.common.Detections'> instance\n",
              "image 1/1: 576x768 1 bicycle, 1 car, 1 dog\n",
              "Speed: 3.0ms pre-process, 7.6ms inference, 3.6ms NMS per image at shape (1, 3, 480, 640)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "jTbYnuqEUujb",
        "outputId": "376b8077-b9d5-4d82-bd60-32cc9aeb3e78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>models.yolo.DetectionModel</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/root/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py</a>YOLOv5 detection model class for object detection tasks, supporting custom configurations and anchors.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 218);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "models.yolo.DetectionModel"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detection_model = model.model.model\n",
        "\n",
        "type(detection_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9C8E6decVxW"
      },
      "outputs": [],
      "source": [
        "# Checkout model config\n",
        "# https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW0DX4yMWBib",
        "outputId": "6a00f134-0968-4249-c584-0b573dbf67e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4, 6, 10, 14, 17, 20, 23]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detection_model.save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJcGeg3LWQYV",
        "outputId": "bb9bbe76-2b6c-4967-89eb-167171a4d027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 \t -1\n",
            "1 \t -1\n",
            "2 \t -1\n",
            "3 \t -1\n",
            "4 \t -1\n",
            "5 \t -1\n",
            "6 \t -1\n",
            "7 \t -1\n",
            "8 \t -1\n",
            "9 \t -1\n",
            "10 \t -1\n",
            "11 \t -1\n",
            "12 \t [-1, 6]\n",
            "13 \t -1\n",
            "14 \t -1\n",
            "15 \t -1\n",
            "16 \t [-1, 4]\n",
            "17 \t -1\n",
            "18 \t -1\n",
            "19 \t [-1, 14]\n",
            "20 \t -1\n",
            "21 \t -1\n",
            "22 \t [-1, 10]\n",
            "23 \t -1\n",
            "24 \t [17, 20, 23]\n"
          ]
        }
      ],
      "source": [
        "for i, m in enumerate(detection_model.model):\n",
        "    print(i, \"\\t\", m.f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QIyK95wYFwp",
        "outputId": "0a4607c5-b458-4a79-87ad-6f0610e3c5a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 576, 768])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "image_t = ToTensor()(image)\n",
        "image_t = image_t.unsqueeze(0).to(device)\n",
        "image_t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUOwtl63Sg1d"
      },
      "outputs": [],
      "source": [
        "# from forward once in Base Model\n",
        "# https://github.com/ultralytics/yolov5/blob/5cdad8922c83b0ed49a0173cd1a8b0739acbb336/models/yolo.py#L161\n",
        "\n",
        "def forward_once(detection_model, x):\n",
        "    y, dt = [], []  # outputs\n",
        "    for m in detection_model.model:\n",
        "        if m.f != -1:  # if not from previous layer\n",
        "            x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n",
        "\n",
        "        x = m(x)  # run\n",
        "        y.append(x if m.i in detection_model.save else None)  # save output\n",
        "\n",
        "    return x\n",
        "\n",
        "forward_result = forward_once(detection_model, image_t)\n",
        "forward_result = forward_result[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtC8YBANYcWz",
        "outputId": "0d30f3e9-bf6b-4fb0-9e2c-e2bf9865129f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 27216, 85])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "forward_result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOair3waacWz",
        "outputId": "d59d4cde-4f20-46fb-9713-079c208d9d62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([11, 85])"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# forward_result ~ [ bs, bbox_num, bbox_params + object confidence + class_probability ]\n",
        "\n",
        "# bbox_params + object confidence = 5 params\n",
        "# class_probability = 80 params\n",
        "\n",
        "# bbox_params = [ cx, cy, w, h ]\n",
        "\n",
        "most_confident_bboxes = forward_result[forward_result[..., 4] > 0.75]\n",
        "most_confident_bboxes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYOskXwbdrkR",
        "outputId": "dc56a9be-3b5c-4a5a-d8f8-1a2d9d110b1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[581., 124., 215., 101.],\n",
              "        [581., 124., 214., 101.],\n",
              "        [578., 123., 216.,  99.],\n",
              "        [579., 123., 211.,  98.],\n",
              "        [579., 124., 212.,  99.],\n",
              "        [578., 123., 215.,  99.],\n",
              "        [579., 123., 211.,  98.],\n",
              "        [579., 124., 212.,  99.],\n",
              "        [578., 123., 216.,  99.],\n",
              "        [579., 123., 211.,  98.],\n",
              "        [579., 123., 212.,  99.]], device='cuda:0')"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "most_confident_bboxes[:, :4].round()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYhYNwN1c6f1"
      },
      "outputs": [],
      "source": [
        "# non_max_suppression\n",
        "# https://github.com/ultralytics/yolov5/blob/5cdad8922c83b0ed49a0173cd1a8b0739acbb336/utils/general.py#L1010"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAl4vXluvBxU"
      },
      "source": [
        "# Аугментации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goa2JLmGvFPC"
      },
      "outputs": [],
      "source": [
        "# Image Classification\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "H, W = 32, 32\n",
        "img = torch.randint(0, 256, size=(3, H, W), dtype=torch.uint8)\n",
        "\n",
        "transforms = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "img = transforms(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok2oMcKt0gzA"
      },
      "source": [
        "# Интерпретация предсказаний сверточных классификаторов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2-qT9zywhw-"
      },
      "source": [
        "| Aspect                  | **Saliency Maps**                                                                                               | **Grad-CAM**                                                                                                                           | **LRP**                                                                                                     |\n",
        "| ----------------------- | --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
        "| **Core Idea**           | Compute the gradient of the output w\\.r.t. each input pixel to identify which pixels influence the output most. | Use gradients and feature maps from the last convolutional layer to highlight important regions.                                       | Backpropagate the prediction score to input features using relevance propagation rules.                     |\n",
        "| **Mechanism**           | - Compute ∂(class score)/∂(input pixel).<br>- Take absolute or max gradient per pixel.                          | - Compute gradients of output w\\.r.t. final conv layer.<br>- Weight feature maps with mean gradient.<br>- ReLU + upsampling = heatmap. | - Use custom backpropagation rules (e.g., ε-rule) to redistribute prediction score through layers to input. |\n",
        "| **Output Type**         | Pixel-level gradient map.<br>Often noisy and high-frequency.                                                    | Coarse, class-discriminative heatmap overlaying important regions.                                                                     | Pixel-level relevance map highlighting specific contributing features.                                      |\n",
        "| **Granularity**         | Fine, but often noisy and lacks class specificity.                                                              | Coarse, but class-specific and visually intuitive.                                                                                     | Fine-grained, class-specific, relevance-focused.                                                            |\n",
        "| **Visual Intuition**    | Sharp but noisy.<br>Can highlight edges or textures unrelated to semantics.                                     | Highlights *where* the model looks (regions).                                                                                          | Highlights *what* pixels contribute most to decision.                                                       |\n",
        "| **Model Compatibility** | Any DNN with differentiable inputs.                                                                             | Requires CNNs (convolutional layers).                                                                                                  | General (MLPs, CNNs, RNNs — with rule tuning).                                                              |\n",
        "| **Computational Cost**  | Low (one backprop).                                                                                             | Moderate (uses gradients + feature maps).                                                                                              | Moderate to high (custom rules for backprop).                                                               |\n",
        "| **Ease of Use**         | Very easy.<br>Basic gradient calculation.                                                                       | Easy in CNNs.<br>Widely supported.                                                                                                     | Complex.<br>Requires rule selection per layer.                                                              |\n",
        "| **Limitations**         | - High-frequency noise.<br>- Not class-specific if multiple classes overlap.                                    | - Low spatial resolution.<br>- Depends on convolutional structure.                                                                     | - Harder to interpret.<br>- Sensitive to propagation rule design.                                           |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ugkcGauUS-"
      },
      "source": [
        "## Class Activation Map\n",
        "\n",
        "Интерпретация предсказаний сверточных нейросеток\n",
        "\n",
        "http://cnnlocalization.csail.mit.edu/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEMxyRw82OpL"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/CAM.png\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN2zvGL8aQ1j"
      },
      "source": [
        "Пояснения к картинке:\n",
        "\n",
        "1. Серые квадраты -- это активации (тензоры) после применения сверток на разных слоях\n",
        "2. Сине-красно-зеленый квадрат -- это последняя активация, перед `GAP`\n",
        "3. `GAP` -- Global Average Polling\n",
        "4. w1, w2, ... -- веса полносвязной сети\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2zoDD7tkHYb",
        "outputId": "95634e87-cf87-42e9-cf51-c0bb0d2c599d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-08 20:35:31--  https://raw.githubusercontent.com/m12sl/dl-hse-2021/main/05-computer-vision-1/imagenet_1k.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31565 (31K) [text/plain]\n",
            "Saving to: ‘imagenet_1k.json’\n",
            "\n",
            "imagenet_1k.json    100%[===================>]  30.83K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-03-08 20:35:32 (25.2 MB/s) - ‘imagenet_1k.json’ saved [31565/31565]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/m12sl/dl-hse-2021/main/05-computer-vision-1/imagenet_1k.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VluJkAo80GnX"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import json\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "with open(\"imagenet_1k.json\") as fin:\n",
        "    clsidx = json.load(fin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dxO_EX9jb_K",
        "outputId": "65ca82bc-ecf3-4825-dafe-669f3b6e25f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 125MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(Sequential(\n",
              "   (0): Bottleneck(\n",
              "     (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): Bottleneck(\n",
              "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (2): Bottleneck(\n",
              "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              " ),\n",
              " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
              " Linear(in_features=2048, out_features=1000, bias=True))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = torchvision.models.resnet50(pretrained=True)\n",
        "net.layer4, net.avgpool, net.fc # 3 last layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn_b2vp3blvO",
        "outputId": "63ac05cd-7a48-429c-ee5f-040a2a946828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-08 20:36:09--  https://horseyhooves.com/wp-content/uploads/2021/09/Brown-horse-and-Yorkshire-Terrier-dog.jpg.webp\n",
            "Resolving horseyhooves.com (horseyhooves.com)... 104.21.91.205, 172.67.179.89, 2606:4700:3031::6815:5bcd, ...\n",
            "Connecting to horseyhooves.com (horseyhooves.com)|104.21.91.205|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42630 (42K) [image/webp]\n",
            "Saving to: ‘Brown-horse-and-Yorkshire-Terrier-dog.jpg.webp.1’\n",
            "\n",
            "\r          Brown-hor   0%[                    ]       0  --.-KB/s               \rBrown-horse-and-Yor 100%[===================>]  41.63K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-03-08 20:36:09 (22.1 MB/s) - ‘Brown-horse-and-Yorkshire-Terrier-dog.jpg.webp.1’ saved [42630/42630]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://horseyhooves.com/wp-content/uploads/2021/09/Brown-horse-and-Yorkshire-Terrier-dog.jpg.webp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gNuZpwlPBN0",
        "outputId": "6abaa0ad-76ef-45c0-a050-6db8a9bb6fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([187])\n"
          ]
        }
      ],
      "source": [
        "IMAGE_PATH = \"./Brown-horse-and-Yorkshire-Terrier-dog.jpg.webp\"\n",
        "\n",
        "def load(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (300, 210))\n",
        "    img = img.astype(np.float32) / 255.0 # img \\in [0, 1]\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
        "    std =  np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n",
        "    img = (img - mean) / std\n",
        "    img = img.astype(np.float32)\n",
        "    img = np.transpose(img, [2, 0, 1])[None, ...]\n",
        "    img = torch.tensor(img)\n",
        "    # [1, 3, w, h]\n",
        "    return img\n",
        "\n",
        "img = load(IMAGE_PATH)\n",
        "\n",
        "with torch.no_grad():\n",
        "    net.eval()\n",
        "    out = net(img)\n",
        "    print(out.argmax(dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuSfnWFGcUCs",
        "outputId": "673dfb45-4da7-48d7-a766-973dacdd75fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 210, 300])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU_EDhcLjxzP",
        "outputId": "248024cb-ddbf-4bae-ff1b-998cb649d1ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "values tensor([[7.3200, 6.3566, 6.1331, 5.8092, 5.4847]])\n",
            "idx.   [187 193 515 186 730]\n"
          ]
        }
      ],
      "source": [
        "values, idx = out.topk(5, dim=-1)\n",
        "idx = idx.numpy().reshape(-1)\n",
        "print(\"values\", values)\n",
        "print(\"idx.  \", idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXUB5L1TkODT",
        "outputId": "ab73c9fd-7bd5-400a-87c4-efea4cca708f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Yorkshire terrier',\n",
              " 'Australian terrier',\n",
              " 'cowboy hat, ten-gallon hat',\n",
              " 'Norwich terrier',\n",
              " 'plow, plough']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[  clsidx[str(l)] for l in idx ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgUaLlxPjo_W",
        "outputId": "69744e3d-6f67-4876-a314-9f480f0592cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2048, 7, 10])\n",
            "torch.Size([1, 7, 10, 2048])\n",
            "torch.Size([1, 7, 10, 1000])\n"
          ]
        }
      ],
      "source": [
        "net = torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "feature_maps = []\n",
        "globalpolling_weights = []\n",
        "def hook_fn(module, inp, out):\n",
        "    feature_maps.append(inp[0])\n",
        "    globalpolling_weights.append(out[0])\n",
        "\n",
        "with torch.no_grad():\n",
        "    net._modules.get(\"avgpool\").register_forward_hook(hook_fn)\n",
        "    net.eval()\n",
        "    out = net(img).numpy()\n",
        "\n",
        "x = feature_maps[0]\n",
        "print(x.shape)\n",
        "\n",
        "with torch.no_grad():\n",
        "    fc = net._modules.get(\"fc\")\n",
        "    xx = x.permute([0, 2, 3, 1])\n",
        "    print(xx.shape)\n",
        "    y = fc(xx)\n",
        "    print(y.shape)\n",
        "    y = y.numpy()[0, ...]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7mR87cB-PidW",
        "outputId": "0a459b3b-8715-40a5-fda6-e284eb3ceb8e"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "original = cv2.imread(IMAGE_PATH)\n",
        "# plt.imshow(original)\n",
        "# print(original.shape)\n",
        "\n",
        "for i in idx:\n",
        "    print(i, clsidx[str(i)])\n",
        "    plt.figure()\n",
        "    cam = y[..., i]\n",
        "    cam /= (cam.max() + 1e-5)\n",
        "    # cam = cam.transpose()\n",
        "    # print(cam.shape)\n",
        "    cam = np.absolute(cam)\n",
        "    cam = cv2.resize(cam, (671, 404))\n",
        "    t = original.astype(np.float32) / 255.0\n",
        "    t[..., 1] = cam\n",
        "    t[..., 2] = cam\n",
        "    plt.imshow(t)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqWRE7WV0Ou_"
      },
      "source": [
        "# Практическая часть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMRRszaHP9NW"
      },
      "source": [
        "# Pytorch Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5czDFUNP-Dh"
      },
      "source": [
        "https://www.pytorchlightning.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THbGA6QovUbs"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "Q3QiTwuZu6xp",
        "outputId": "619b1f64-1e65-4f84-da5f-563f7f39ac9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-174d021a-c776-4415-b815-4b9f053c3f60\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-174d021a-c776-4415-b815-4b9f053c3f60\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 63 bytes\n"
          ]
        }
      ],
      "source": [
        "# How to use the Kaggle API from Colab\n",
        "# https://colab.research.google.com/github/corrieann/kaggle/blob/master/kaggle_api_in_colab.ipynb\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwZOoR-qYqt3",
        "outputId": "044aea2d-0092-48c4-8704-77f8bb17d62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 223544\n",
            "drwxr-xr-x 3 root root      4096 Feb 13 15:25 flower-photos-classification\n",
            "-rw-r--r-- 1 root root 228895658 Feb 13 15:25 flower-photos-classification.zip\n",
            "drwxr-xr-x 1 root root      4096 Feb  9 14:42 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gL9XJqVRV99"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c flower-photos-classification -q\n",
        "# !mv flower_photos.zip flower-photos-classification.zip\n",
        "!unzip -fq flower-photos-classification.zip -d flower-photos-classification\n",
        "!rm -rf flower_photos\n",
        "!tar -zxf flower-photos-classification/flower_photos.tgz --directory flower-photos-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJByuBIcS3_A",
        "outputId": "64110ac7-1054-4789-b3f4-ae2ecada06ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 223500\n",
            "drwxr-x--- 7 270850 5000      4096 Feb 10  2016 flower_photos\n",
            "-rw-r--r-- 1 root   root 228813984 May  7  2021 flower_photos.tgz\n",
            "-rw-r--r-- 1 root   root     41780 May  7  2021 sample_submission.csv\n",
            "total 612\n",
            "drwx------ 2 270850 5000  36864 Feb 10  2016 daisy\n",
            "drwx------ 2 270850 5000  53248 Feb 10  2016 dandelion\n",
            "-rw-r----- 1 270850 5000 418049 Feb  9  2016 LICENSE.txt\n",
            "drwx------ 2 270850 5000  36864 Feb 10  2016 roses\n",
            "drwx------ 2 270850 5000  36864 Feb 10  2016 sunflowers\n",
            "drwx------ 2 270850 5000  40960 Feb 10  2016 tulips\n"
          ]
        }
      ],
      "source": [
        "!ls -l flower-photos-classification/\n",
        "!ls -l flower-photos-classification/flower_photos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcaeFDeivisI",
        "outputId": "2dcc3695-7665-4558-f29b-668fe15ead96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pytorch-lightning\n",
        "!pip install -q efficientnet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4_zhGQtS37S"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "from IPython.display import FileLink\n",
        "from copy import deepcopy\n",
        "import os\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torchvision\n",
        "from efficientnet_pytorch import EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjIYRhgsS32_",
        "outputId": "716861ef-5a1b-4b22-d28c-4f913c918dfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajqLeJcCS3zF",
        "outputId": "0c220cc5-9e53-4ce0-defa-a951090404fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(213, 320, 3)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plt.imread('flower-photos-classification/flower_photos/daisy/105806915_a9c13e2106_n.jpg').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhzBrycoS3wV"
      },
      "outputs": [],
      "source": [
        "# [ x for x in test_answers['Id'][:3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hRERwjKjcpx",
        "outputId": "2adbe0ed-934d-47df-9857-2013d48f23b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 612\n",
            "drwx------ 2 270850 5000  36864 Feb 10  2016 daisy\n",
            "drwx------ 2 270850 5000  53248 Feb 10  2016 dandelion\n",
            "-rw-r----- 1 270850 5000 418049 Feb  9  2016 LICENSE.txt\n",
            "drwx------ 2 270850 5000  36864 Feb 10  2016 roses\n",
            "drwx------ 2 270850 5000  36864 Feb 10  2016 sunflowers\n",
            "drwx------ 2 270850 5000  40960 Feb 10  2016 tulips\n"
          ]
        }
      ],
      "source": [
        "!ls -l flower-photos-classification/flower_photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcxQ90g9wZNG",
        "outputId": "ecee1d19-5094-435f-f39f-2223b5d50b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 34796\n",
            "-rw-r----- 1 270850 5000  26797 Jan 11  2016 100080576_f52e8ee070_n.jpg\n",
            "-rw-r----- 1 270850 5000 117247 Jan 11  2016 10140303196_b88d3d6cec.jpg\n",
            "-rw-r----- 1 270850 5000  36410 Jan 11  2016 10172379554_b296050f82_n.jpg\n",
            "-rw-r----- 1 270850 5000 102862 Jan 11  2016 10172567486_2748826a8b.jpg\n",
            "-rw-r----- 1 270850 5000  27419 Jan 11  2016 10172636503_21bededa75_n.jpg\n",
            "-rw-r----- 1 270850 5000 132803 Jan 11  2016 102841525_bd6628ae3c.jpg\n",
            "-rw-r----- 1 270850 5000 102618 Jan 11  2016 1031799732_e7f4008c03.jpg\n",
            "-rw-r----- 1 270850 5000  51688 Jan 11  2016 10391248763_1d16681106_n.jpg\n",
            "-rw-r----- 1 270850 5000  13946 Jan 11  2016 10437754174_22ec990b77_m.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls -l flower-photos-classification/flower_photos/daisy | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtndZbuSxD41"
      },
      "outputs": [],
      "source": [
        "!rm -rf flower-photos-classification/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnEtmE7fS3t1"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = [128, 128]\n",
        "\n",
        "class FlowerDataModlule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=50, valid_ratio=0.05):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.valid_ratio = valid_ratio\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        test_transforms = [\n",
        "            torchvision.transforms.Resize([128, 128]),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "        ]\n",
        "\n",
        "        train_transforms = deepcopy(test_transforms)\n",
        "        train_transforms.append( torchvision.transforms.RandomAffine( degrees=10, scale=(0.9, 1.1)) )\n",
        "\n",
        "        test_transforms = torchvision.transforms.Compose(test_transforms)\n",
        "        train_transforms = torchvision.transforms.Compose(train_transforms)\n",
        "\n",
        "        all_train_data = torchvision.datasets.ImageFolder(\n",
        "            'flower-photos-classification/flower_photos/',\n",
        "            transform=train_transforms,\n",
        "        )\n",
        "\n",
        "\n",
        "        valid_len = int(self.valid_ratio * len(all_train_data))\n",
        "        train_len = len(all_train_data) - valid_len\n",
        "        train_data, valid_data = torch.utils.data.random_split(all_train_data, (train_len, valid_len))\n",
        "        self.train = train_data\n",
        "        self.valid = valid_data\n",
        "\n",
        "\n",
        "        test_dir = 'flower-photos-classification/test/'\n",
        "        if not os.path.exists(test_dir):\n",
        "            # print(\"generating test dataset\")\n",
        "            os.mkdir(test_dir)\n",
        "            test_dir_with_label = test_dir + 'fake_label'\n",
        "            os.mkdir(test_dir_with_label)\n",
        "\n",
        "\n",
        "            test_answers = pd.read_csv(\"flower-photos-classification/sample_submission.csv\")\n",
        "            # print(\"test_answers\", test_answers)\n",
        "            for src in test_answers['Id']:\n",
        "                fname = os.path.basename(src)\n",
        "                # print(src, os.path.join(test_dir_with_label, fname))\n",
        "                copyfile(os.path.join(\"flower-photos-classification/\", src), os.path.join(test_dir_with_label, fname))\n",
        "\n",
        "        self.test = torchvision.datasets.ImageFolder(test_dir, transform=test_transforms)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train, shuffle=True, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.valid, shuffle=False, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test, shuffle=False, batch_size=self.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DERDUd3UzWm",
        "outputId": "dada0cf7-48b4-4667-e5ba-f08eec346c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}\n"
          ]
        }
      ],
      "source": [
        "cdm = FlowerDataModlule(batch_size=10)\n",
        "cdm.setup()\n",
        "print(cdm.train.dataset.class_to_idx)\n",
        "class_index_to_label = [\n",
        "    \"DAISY\",\n",
        "    \"DANDELION\",\n",
        "    \"ROSE\",\n",
        "    \"SUNFLOWER\",\n",
        "    \"TULIP\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hViuTSSUUzfB"
      },
      "outputs": [],
      "source": [
        "class FlowerClf(pl.LightningModule):\n",
        "    def __init__(self, labels_cnt=5, lr=3e-4, train_fc_only_steps=100, train_tail_only_steps=2000, train_with_half_blocks_steps=3000):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.labels_cnt = labels_cnt\n",
        "\n",
        "        self.lr = lr\n",
        "        self.train_fc_only_steps = train_fc_only_steps\n",
        "        self.train_tail_only_steps = train_tail_only_steps\n",
        "        self.train_with_half_blocks_steps = train_with_half_blocks_steps\n",
        "\n",
        "        self._setup_model()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        return\n",
        "\n",
        "    def _setup_model(self):\n",
        "        self.model = EfficientNet.from_pretrained('efficientnet-b6')\n",
        "        orig_model_fc = self.model._fc\n",
        "        self.model._fc = nn.Linear(orig_model_fc.in_features, self.labels_cnt)\n",
        "\n",
        "    def freeze(self, module):\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def unfreeze(self, module):\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    def gradual_unfreeze(self):\n",
        "\n",
        "        if self.trainer.global_step < self.train_fc_only_steps:\n",
        "            self.freeze(self.model)\n",
        "            self.unfreeze(self.model._fc)\n",
        "        elif self.trainer.global_step < self.train_tail_only_steps:\n",
        "            self.unfreeze(self.model)\n",
        "            self.freeze(self.model._conv_stem)\n",
        "            self.freeze(self.model._bn0)\n",
        "            self.freeze(self.model._blocks)\n",
        "        elif self.trainer.global_step < self.train_with_half_blocks_steps:\n",
        "            self.unfreeze(self.model)\n",
        "            self.freeze(self.model._conv_stem)\n",
        "            self.freeze(self.model._bn0)\n",
        "            for mod in self.model._blocks[:22]:\n",
        "                self.freeze(mod)\n",
        "        else:\n",
        "            self.unfreeze(self.model)\n",
        "\n",
        "        # https://github.com/PyTorchLightning/pytorch-lightning/issues/2733\n",
        "        # pip install pytorch-lightning==0.9.0rc2\n",
        "        # self.log(\"trainable_params\", pl.core.memory.ModelSummary(self).trainable_parameters, prog_bar=True)\n",
        "\n",
        "    def compute_loss(self, batch, track_accuracy=False):\n",
        "\n",
        "        images, labels = batch\n",
        "        class_probas = self.model.forward(images)\n",
        "\n",
        "        loss = self.criterion(class_probas, labels.view(-1))\n",
        "\n",
        "        if track_accuracy:\n",
        "            _, predicted_labels = class_probas.max(dim=-1)\n",
        "\n",
        "            accuracy = (predicted_labels == labels).sum() / labels.numel()\n",
        "            self.log('accuracy', accuracy.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        self.gradual_unfreeze()\n",
        "\n",
        "        loss = self.compute_loss(batch)\n",
        "\n",
        "        self.log('loss', loss.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "\n",
        "        loss = self.compute_loss(batch, track_accuracy=True)\n",
        "\n",
        "        self.log('valid_loss', loss.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # def val_epoch_end(self, validation_step_outputs):\n",
        "        # pass\n",
        "        #\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, _ = batch\n",
        "        class_probas = self.model.forward(images)\n",
        "        _, predicted_labels = class_probas.max(dim=-1)\n",
        "\n",
        "        return predicted_labels\n",
        "\n",
        "\n",
        "    def test_epoch_end(self, train_steps_outs):\n",
        "\n",
        "        cated_train_steps_outs = torch.cat(train_steps_outs, dim=0)\n",
        "\n",
        "        predicted_labels = cated_train_steps_outs.cpu().detach().numpy()\n",
        "        test_answers = pd.read_csv(\"flower-photos-classification/sample_submission.csv\")\n",
        "        category_prediction = pd.DataFrame({\n",
        "            \"Id\": test_answers['Id'],\n",
        "            \"Category\": [ class_index_to_label[x] for x in predicted_labels.tolist()]\n",
        "        })\n",
        "\n",
        "\n",
        "        csv_file = f'flower-photos-classification/mysubmission_v{self.trainer.logger.version}.csv'\n",
        "        category_prediction.to_csv(csv_file, index=False)\n",
        "        self.csv_file_link = FileLink(csv_file)\n",
        "        print(\"csv_file_link:\", self.csv_file_link)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        lr_schedulers = {'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=10, verbose=True), 'monitor': 'loss'}\n",
        "        return [opt], lr_schedulers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zvkbV8vx95h"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9lJaOvRx8nC"
      },
      "outputs": [],
      "source": [
        "!mkdir ./lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxPUyEBbJ73w"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir ./lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523,
          "referenced_widgets": [
            "776f38b9296945faa6dcac4119fc4b39",
            "851d7a63408e44a48d0d90614ccda1e9",
            "fb1b5d8f5a53478da6e3cb1406531afa",
            "734757b477554e1ab6e9c97ae88838c1",
            "f1e70135638442778c4495d958861571",
            "2dca53d4e25c4054a3bed3627d0560c0",
            "70f32a66edfc4ddaaaadd3481f2ef2e8",
            "6685106bfd8c42a58c2a5d710d2c99e6",
            "68ff38613fa7421f9d323f4a0471215a",
            "e8036ef40d8a4e439a731d1d80baebd6",
            "154914127e9d472f97f41747c0f3b8a5",
            "7b27c17fe42449aa91ac7f16419675dc",
            "981ae57074f84ed9b6e4fb26c1762fdb",
            "9e4be6277e5743c280f75166d1922d88",
            "f74c40354cdc4307b8f4c2dd4e256ffe",
            "2439550017f54e44805043653c34d4de",
            "47de4ec66f7e4cd587afd22bdca445c0",
            "c8e4347c21014f0d858fa34790937547",
            "ce002aaca0e5452c94a917d433280376",
            "7866a253689d42ff938657261218e0a1",
            "44500603531e46cf81edb17674220204",
            "f91d792c725e4dc9a7ed2fd7c0919c1d",
            "d944cd8c81ab4191a22943e531ad8a54",
            "fbf26800648e40508d0b825e92a7b43e",
            "eff1e1a889c44166b604913e0232cf7c",
            "c5519a0c346f474caf46f6a68cd71973",
            "c688fc441c144ca49a2d23fb453a9b25",
            "399fa5e99c114e51b0cb49352e208dfb",
            "8f1005e7b5d74c64a950dad0223bec11",
            "b1d04552793b49d0bb7828c74c487c67",
            "db6122b76a9e4a4184bd11b965b8b937",
            "d1e81886f59840a582fc101990522d14",
            "d97d0e168059439d8ca5e4b4b6dd527d",
            "f1ccb06efd3742389d4df098fe9500cc",
            "e04d9af0a2d0434c83ab2f0d8643e7fb",
            "9bd1241776674e8d81b5fbc9c715bb04",
            "489ed244b93e4c55a6beefa93e4e52fe",
            "7c577477095543ceb7a1932ccd802a04",
            "505adb42c37f440baac98351e16ededa",
            "2dd654d548344a28b9a1f41842b2a941",
            "b94e5b60777b4c7ba9e55c49fd0f9faa",
            "dd6773ac741c44f193caaa1863ab4797",
            "a8c2b66fcb1a4ff3b94ac63f2b4812ec",
            "5df5f96e8cef434ca0d19809cc39da3b",
            "8bd70b6e4e1448f3a49e7de8de61735d",
            "cd4c6985230d424497b4b752347b3f1c",
            "81fe4e102a874a95be2717295fdde30f",
            "f56f1ac681c24e0e8c2d06b87b5cfba8",
            "a0322610ec6e4756964b320d8c39937c",
            "91f53d3db5f142d28c395a228fce8e34",
            "0e6c2e9175fb48ef85904366abde96bf",
            "3861b2e3ad40460abb832ddacf94b471",
            "96453fdc3cd5447885b565122274179f",
            "1842ed0eef754d238e8bb6b0add999c3",
            "a705b6e798704c2c84a98c347c7fdfb2",
            "cb297c19e9134e2ca8ab83d484eea1d9",
            "17aabdf515e44ce29131c6db8eaaf100",
            "938be345361f4cd8b502ac22fa4a957d",
            "4a41a1211c704778bde3de0050e236d9",
            "850ce03129af4eaf9988e524e82b9a94",
            "8b669718d0a44c5292592788531354a8",
            "6e1b5e8e4b5a4b67aa332dde0e4bed38",
            "08980cfefc8544739ce00f0a6072236e",
            "1517cbc5e0e94a239f2938c843f65a78",
            "b0990c1817b84624bba912ee4dc89417",
            "c01a6a58759648a1982acdb022dc0934"
          ]
        },
        "id": "41oCdYHuUzmK",
        "outputId": "2603b64c-6ee2-44a2-a458-c4789fb67d20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b6-c76e70fd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b6-c76e70fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "776f38b9296945faa6dcac4119fc4b39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/165M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:466: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | EfficientNet     | 40.7 M\n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "40.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "40.7 M    Total params\n",
            "162.989   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b27c17fe42449aa91ac7f16419675dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d944cd8c81ab4191a22943e531ad8a54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1ccb06efd3742389d4df098fe9500cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.callbacks.early_stopping:Metric accuracy improved. New best score: 0.301\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bd70b6e4e1448f3a49e7de8de61735d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.callbacks.early_stopping:Metric accuracy improved by 0.169 >= min_delta = 0.01. New best score: 0.470\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb297c19e9134e2ca8ab83d484eea1d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.callbacks.early_stopping:Metric accuracy improved by 0.022 >= min_delta = 0.01. New best score: 0.492\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
          ]
        }
      ],
      "source": [
        "cdm = FlowerDataModlule(batch_size=10)\n",
        "cplm = FlowerClf(lr=0.00001)\n",
        "\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='accuracy',\n",
        "    mode='max',\n",
        "    min_delta=0.01,\n",
        "    patience=2,\n",
        "    verbose=True,\n",
        ")\n",
        "trainer = pl.Trainer(gpus=1, callbacks=[early_stop_callback], max_epochs=3, track_grad_norm=2)\n",
        "\n",
        "trainer.fit(cplm, cdm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9kpO1cnzq7Y"
      },
      "outputs": [],
      "source": [
        "# наследуемся от предыдущей модели\n",
        "class FlowerClfResNet(FlowerClf):\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def _setup_model(self):\n",
        "        self.model = torchvision.models.resnet50(pretrained=True)\n",
        "        orig_model_fc = self.model.fc\n",
        "        self.model.fc = nn.Linear(orig_model_fc.in_features, self.labels_cnt)\n",
        "\n",
        "    def gradual_unfreeze(self):\n",
        "\n",
        "        if self.trainer.global_step < self.train_fc_only_steps:\n",
        "            self.freeze(self.model)\n",
        "            self.unfreeze(self.model.fc)\n",
        "        else:\n",
        "            self.unfreeze(self.model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdko94A5Ik0x"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "tb_logger = pl_loggers.TensorBoardLogger(\"./lightning_logs/\", version=\"resnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560,
          "referenced_widgets": [
            "98b4e96e8d304f75bae8608a7f7ca5fd",
            "46d652fbc0a8462fa0ab3000a3fb2317",
            "d6c7fb2d4d464d17bcc2c784d12c7832",
            "9a06a63809eb479ab36b70d0becb94cf",
            "264e638231a247e895c201f348c3bb1e",
            "f99e563335aa45bc81ded0881524d97e",
            "f5cac9e08a4940db9c7eda04c3b5afae",
            "77b87cbfa2f3496fb38f51a5b8940053",
            "45350e359c6c4b2c9a46ba08084634de",
            "0d2037fdb0ac4907a79d21c8facfadf3",
            "4270b12e1e0849b299bcb056fdab31eb",
            "d9e151416c554f15ad4bd517c3c9206d",
            "5d596aae834a4ecc8e1b063d98c8f266",
            "44d82e2b89814ed8b417d0c4de17a5b0",
            "2ec80402496245f69d03631d684b6f41",
            "9c0b1080f38945879ead9b8418a343df",
            "d69ce4c436f44d87a25eae325dfe8a05",
            "22e7333e43874cef9e9a5aeb01337548",
            "9e4b53056e08492c97daaf4a44f9e975",
            "f40aa2e0b1e74e1e8dbf9589cb9c1b75",
            "c45bd90d931c4a4d929080c86c40fa4d",
            "7a8b11063f28467b8cc7215ad335a33b",
            "8d42851fcfd84d4f94b9193a8e76903a",
            "42f9d29ac00b4647a531c3666563a86d",
            "61607e8ccd324a9cb35133a619f3f6d6",
            "6dee4b77e56244fa83fe60fe7d5f4108",
            "9c67d5e7d0384f6ea147e66b7ef81e64",
            "4db98c7094584aa7979c8a9e66cb597f",
            "46be69789869402b801b9e78583df3c3",
            "ccf87a58d4874c4688714c051a9c3cea",
            "5c143cb0600446638bb44081dbf78004",
            "3d19c4baf8834ce084980f7da2e48e21",
            "580fd0c815da451090136ec0246f1894",
            "7badacfa3a4f4532a043fc94cba8bc54",
            "b97dac5540544d478ae7ceaccc4c47d4",
            "df4d0b9dc19c4aa5a43f029c2d31f80f",
            "a418ebacf6d74b3d8997ad5bebc97014",
            "2c0b2a295e3642899610a03c81307125",
            "c3103c6ff5d84817a36a5d453dd1c837",
            "b80947ffb71041009fd21af72f6d62c5",
            "3a698f0d5d9d42d08727467bf44dfe6d",
            "53f014e0d9d74a478f53ab38806e6e54",
            "63a7b1570a3244d78bc4a4b5e02f230a",
            "9dfcff93fc5e4f37bb445e6de0a70c0c",
            "6677ae0420384d6b96beaa0e21cbf34e",
            "811085e18505419d9d2811d6dab69290",
            "5cf5bafbca0e486cb55df77b64e98a36",
            "ee1bb9f0c9bf40ff947a78b4fb433ab6",
            "aa64ddd886424bac86d3d6dc32a0466d",
            "2aa013a230a041568f74fe1451b5e96e",
            "9071c74d1a484de9afa7a9eb87a885b7",
            "461e03043fff47dead1c03268b53d578",
            "f383178120d44790a49b36bd7f9c3e2c",
            "b02e2cbb58794945b753e72a2e91950c",
            "a4a28b44492c4d11b0bfeac7d6898c34",
            "b78fb351c81f47c19df6c0546b2ee568",
            "6985a5f7c75b4e628192931ada85d22c",
            "16828593ea014df3acbd0a4d94843619",
            "b2bc336393b94655a751c4b8c2b16b2c",
            "33160870987948288e303910d029018e",
            "90cc642bd6f148bbbf28d060f145b543",
            "2e933d661083469fba3ea5517706aa25",
            "4300f279944d417e87deb2152a33d4b8",
            "cc16b5e1db124a618188b6c18694e584",
            "2c032f3d051b42b7af97eb9abff077aa",
            "4ccb4814db0f48cdb72ec3892add0053"
          ]
        },
        "id": "-JTsdsSvz-iu",
        "outputId": "96217cb6-8155-4cc6-ca36-4a60fc6f9ed0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98b4e96e8d304f75bae8608a7f7ca5fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:466: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | ResNet           | 23.5 M\n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "23.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "23.5 M    Total params\n",
            "94.073    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9e151416c554f15ad4bd517c3c9206d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d42851fcfd84d4f94b9193a8e76903a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7badacfa3a4f4532a043fc94cba8bc54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.callbacks.early_stopping:Metric accuracy improved by 0.361 >= min_delta = 0.01. New best score: 0.852\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6677ae0420384d6b96beaa0e21cbf34e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.callbacks.early_stopping:Metric accuracy improved by 0.033 >= min_delta = 0.01. New best score: 0.885\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b78fb351c81f47c19df6c0546b2ee568",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
          ]
        }
      ],
      "source": [
        "flower_clf_resnet = FlowerClfResNet(lr=0.0001)\n",
        "\n",
        "trainer = pl.Trainer(gpus=1, callbacks=[early_stop_callback], max_epochs=3, logger=tb_logger)\n",
        "trainer.fit(flower_clf_resnet, cdm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJYbsvUd0Ceo"
      },
      "source": [
        "### find_lr\n",
        "\n",
        "Плохо работает, если сетка заморожена, но пользоваться можно как-то так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271,
          "referenced_widgets": [
            "ee8f12b63d9c4fcba81b83bc348ee35f",
            "cf8c927c02d54dc0ad6d7d40ec95cced",
            "c301ef48de11441b89f4bc57e049d664",
            "fce68e3e039e479e9edb6c50c2a4da44",
            "38370ddce8cf468395925b672dc82f38",
            "8a05d169aa694f5fbc1999865e772d49",
            "f0275d503ec845f195dc7c5460dfad7a",
            "99dca452b93140299402224116f51b0c",
            "ba3d1bc23584401bb7cd19d4dad6c4ed",
            "48b5c67abcbd40368aa660c9245be22c",
            "2ccb0618a2c2496c9ec14caeb0f06b3f"
          ]
        },
        "id": "_9k3KddI0J6m",
        "outputId": "0f4e0e66-ca6e-4fbb-b9e5-8f1d85835d4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee8f12b63d9c4fcba81b83bc348ee35f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.tuner.lr_finder:LR finder stopped early after 88 steps due to diverging loss.\n",
            "INFO:pytorch_lightning.tuner.lr_finder:Learning rate set to 2.7542287033381663e-05\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/.lr_find_553bfc39-6c31-4e85-bcbd-97920412b6b3.ckpt\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Restored all states from the checkpoint file at /content/.lr_find_553bfc39-6c31-4e85-bcbd-97920412b6b3.ckpt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'lr_find': <pytorch_lightning.tuner.lr_finder._LRFinder at 0x7f15ee954b20>}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = pl.Trainer(auto_lr_find=True)\n",
        "\n",
        "trainer.tune(flower_clf_resnet, cdm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyetFUy9lqGZ"
      },
      "source": [
        "## Логирование нормы градиентов\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFnn2v1mlt4V"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    callbacks=[early_stop_callback],\n",
        "    max_epochs=3,\n",
        "    track_grad_norm=2, # HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzhQz29tlzP0"
      },
      "source": [
        "# Самодельное логирование нормы градиентов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFjWFuOTmR3N"
      },
      "source": [
        "#### Хуки\n",
        "\n",
        "[register_forward_hook](https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html)\n",
        "\n",
        "Registers a global forward hook for all the modules\n",
        "\n",
        "\n",
        "❗️ Не работают если явно вызывать forward, а не через `__call__` `model()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkiISNmnl8Rb"
      },
      "outputs": [],
      "source": [
        "net = MyNet()\n",
        "\n",
        "def log_grad_norm_hook_fn(module, grad_input, grad_output):\n",
        "    # print(\"backward\", module, grad_input[0].shape, grad_output[0].shape)\n",
        "\n",
        "    grad_norm = grad_output[0].norm(2).item()\n",
        "    trainer.log({'train/grad_norm_' + str(module.__class__): grad_norm})\n",
        "\n",
        "for name, layer in net._modules.items():\n",
        "    layer.register_backward_hook(log_grad_norm_hook_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTFVZ5FSji9q"
      },
      "source": [
        "# Выбор оптимального LR\n",
        "\n",
        "\n",
        "Для выбора оптимального LR удобно использовать т.н. Learning Rate Range Test, часто процедуру называют просто find_lr. Под капотом проход по тренировочной эпохе с lr, изменяемым на каждом батче по формуле:\n",
        "\n",
        "$$\n",
        "\\mathrm{it} = \\frac{\\mathrm{step}}{\\mathrm{total steps}}\\\\\n",
        "\\mathrm{lr} = \\exp\\left\\{\n",
        "    (1 - t ) \\log a + t \\log b\n",
        "\\right\\}\n",
        "$$\n",
        "\n",
        "Чтобы поменять LR для всех оптимизируемых параметров, можно пройтись по ним циклом:\n",
        "\n",
        "```python\n",
        "for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "```\n",
        "\n",
        "\n",
        "<img src=\"https://www.jeremyjordan.me/content/images/2018/02/lr_finder.png\" width=400/>\n",
        "\n",
        "_картинка из блога [Jeremy Jordan](https://www.jeremyjordan.me/nn-learning-rate/)_\n",
        "\n",
        "\n",
        "Идея приема простая: пока LR меньше некоторого порога на каждом шаге градиентного спуска веса просто не меняются (в частности из-за особенностей операций с плавающей точкой).\n",
        "При очень большом LR мы шагаем слишком далеко и уходим от точки экстремума.\n",
        "\n",
        "Оптимальный LR лежит где-то между ними. Экспоненциальная формула изменения LR позволяет с должным качеством найти хорошую точку.\n",
        "\n",
        "\n",
        "\n",
        "[Cтатья , в которой эту технику предложили и активно использовали](https://arxiv.org/pdf/1506.01186.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYZsKcTz-b-X"
      },
      "source": [
        "# Блиц"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnhEdnv--eZ1"
      },
      "source": [
        "### Почему происходит затухание градиентов?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK6b8WI8NTaB"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/vanishing_gradients.png\" width=400 />\n",
        "\n",
        "Слайд из лекции Тани Гайнцевой dlschool\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep2zRtLo-igM"
      },
      "source": [
        "### Какие есть методы борьбы с затуханием градиентов?\n",
        "\n",
        "* residual block\n",
        "* трейнить несколько классификаторов на разных уровнях сети (это старый подход, не актуально в наше время)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpIYnqGN-idl"
      },
      "source": [
        "## Зачем нужен параметр `groups=?` в `nn.Conv2d`?\n",
        "\n",
        "Позволяет уменьшить количество параметров в модели, с помощью него можно регулировать количество обучаемых ядер. Но за счет уменьшения, что не все out_channels теперь имеют информацию только о некоторых in_channels.\n",
        "\n",
        "Если `groups=1`, то это обычная свертка, где все in_channels сворачиваются во все `out_channels`\n",
        "\n",
        "Если `groups=in_channels=out_channels=N`, то такая свертка эквивалентна применению `N` сверток независимо\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TCZcFs1tic7"
      },
      "source": [
        "## Что делать, если во время обучения нейросети начали взрываться градиенты, появились бесконечности в весах модельки?\n",
        "\n",
        "\n",
        "Дебажить) Можно залогировать нормы градиентов для каждого слоя сети.\n",
        "\n",
        "Можно поставить forward/backward хуки, проверять в них, на каком именно слое начали появляться плохие значения или градиенты\n",
        "\n",
        "Воспользоваться `torch.autograd.detect_anomaly`\n",
        "```\n",
        "torch.autograd.detect_anomaly()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwhinV4uP9Ps"
      },
      "source": [
        "------\n",
        "\n",
        "\n",
        "## Вопросы по домашкам?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PmuNUt2KtRt"
      },
      "source": [
        "## Новая домашка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frehHiEsKvHB"
      },
      "source": [
        "# hw-letters (бонусная)\n",
        "\n",
        "Из фидбэка по домашке в прошлом году, эта домашка была очень сложная и очень много времени заняла (35-72 часа), но тем людям, которые оставили фидбэк она понравилась.\n",
        "\n",
        "К решению задачи надо подойти творчески.\n",
        "\n",
        "Сложность домашки заключается в следующем:\n",
        "* достаточно большой объем данных, который надо грамотно предобработать\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y_jZa8ES5qu"
      },
      "source": [
        "<img src=\"https://github.com/fintech-dl-hse/course/raw/refs/heads/main/seminars/05_cv/static/letters_demo.png\" width=400 />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5tQCp17P_UW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YxPuSAPpnds5"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
